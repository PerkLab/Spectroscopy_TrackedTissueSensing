{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ec0cde3",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fee241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all of the requried libraries\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# import statistics\n",
    "from statistics import mode,mean\n",
    "from scipy import interpolate\n",
    "import os\n",
    "import os.path\n",
    "import json\n",
    "import time\n",
    "\n",
    "# These are all of the libraries that I manually created\n",
    "\n",
    "import IOfunctions as IO\n",
    "import GUIfunctions as GUI\n",
    "import Processfunctions as process\n",
    "\n",
    "# Through 3D slicer\n",
    "# start_index = 0 # starts at 195nm\n",
    "# start_index = 742 # starts at 350nm\n",
    "start_index = 790 # starts at 360nm\n",
    "# start_index = 1070 # starts at 420nm\n",
    "\n",
    "# print pandas version\n",
    "print(pd.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cff5d15b",
   "metadata": {},
   "source": [
    "## Data Loading and Formatting \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4a8020e",
   "metadata": {},
   "source": [
    "#### Load, Format, and Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea877aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function combines loading the data with \n",
    "def loadDataset(dataPath,start_index=790,end_index=-1,sep=','):\n",
    "    '''\n",
    "    Function: loadDataset\n",
    "    ---------------------\n",
    "    This function loads in the data from the dataPath, records the time each sample was taken, and returns the dataset as a numpy array.\n",
    "    '''\n",
    "    Dataset = []\n",
    "    Time = []\n",
    "    print(\"Loading in: \", dataPath)\n",
    "    for name in os.listdir(dataPath):\n",
    "        # print(name)\n",
    "        df = pd.read_csv(os.path.join(dataPath,name), sep=sep,engine='python', header=None)\n",
    "        # trim to 360 nm\n",
    "        df = df.iloc[:, start_index:]\n",
    "        # convert to an array\n",
    "        data_arr = df.to_numpy()\n",
    "        # Sum the columns of the array\n",
    "#         spectrum_arr = np.sum(data_arr[1:, 1:],axis=0)\n",
    "        spectrum_arr = np.mean(data_arr[1:, 1:],axis=0)\n",
    "        # Grab the wavelength values\n",
    "        wavelength_arr = data_arr[0, 1:]\n",
    "        # Concatenate the vectors as columns\n",
    "        data_arr = np.concatenate((wavelength_arr.reshape(-1,1), spectrum_arr.reshape(-1,1)), axis=1)\n",
    "        # append to the dataset\n",
    "        Dataset.append(data_arr)\n",
    "\n",
    "        # Get the time of the sample\n",
    "        ctime = os.path.getmtime(os.path.join(dataPath,name))\n",
    "        Time.append(ctime)\n",
    "    Dataset = np.array(Dataset,dtype='float')\n",
    "    Time = np.array(Time,dtype='float')\n",
    "    # print(\"Time shape: \", Time.shape)\n",
    "    return Dataset, Time\n",
    "\n",
    "# LOADING DATASET \n",
    "# FORMAT_DATASET = True\n",
    "FORMAT_DATASET = False\n",
    "dataset_name = 'KidneyData_march3'\n",
    "trialPath = \"C:/Users/David/OneDrive - Queen's University/1 Graduate Studies/1 Thesis Research/KidneyData_march3/March3_KidneyCollectionWithDrRen/Mar03\"\n",
    "sampleNameList = [f for f in os.listdir(trialPath) if f.startswith('Patient')]\n",
    "\n",
    "# define a pandas df to store the incoming data\n",
    "class0_name = 'Normal'\n",
    "class1_name = 'Cancer'\n",
    "\n",
    "if FORMAT_DATASET:\n",
    "    Formatted_dataset_df = pd.DataFrame(columns=['PatientID', 'SampleID', 'Label (numeric)', 'Label', 'Data', 'Time'])  \n",
    "    for sampleName in sampleNameList:\n",
    "        patientID = sampleName.split('_')[0]\n",
    "        sampleID = sampleName.split('_')[1] + '_' +sampleName.split('_')[2]\n",
    "        # define the class names for folders which start with cancer or normal\n",
    "        classNameList = [f for f in os.listdir(os.path.join(trialPath,sampleName)) if f.startswith('Cancer') or f.startswith('Normal')]\n",
    "        # Remove names containing AmbientLight\n",
    "        classNameList = [f for f in classNameList if not f.endswith('AmbientLight')]\n",
    "        # for each folder sampleName folder, check if the class folders exist\n",
    "        for className in classNameList:\n",
    "            # if className contains cancer, then label = 1\n",
    "            if class0_name in className:\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "            # Check to see if the path exists\n",
    "            dataPath = os.path.join(trialPath,sampleName,className)\n",
    "            if os.path.exists(dataPath):\n",
    "                data, time = loadDataset(dataPath,start_index=start_index, sep=',')\n",
    "                # for each data file, append to the dataset\n",
    "                for i in range(data.shape[0]):\n",
    "                    new_row = {'PatientID':patientID, \n",
    "                            'SampleID':sampleID, \n",
    "                            'Label (numeric)':label, \n",
    "                            'Label':className, \n",
    "                            'Data':data[i,:,:],\n",
    "                            'Time': time[i]\n",
    "                    }\n",
    "                    # Dataset_df = Dataset_df.append(new_row, ignore_index=True)\n",
    "                    Formatted_dataset_df = pd.concat([Formatted_dataset_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    # For each Data, convert the array to a string and save it to a csv file\n",
    "    Formatted_dataset_df['Data'] = Formatted_dataset_df['Data'].apply(lambda x: json.dumps(x.tolist()))\n",
    "    file_name = os.path.join(trialPath, dataset_name + '_Formatted_Dataset.csv')\n",
    "    Formatted_dataset_df.to_csv(file_name, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "103f86d9",
   "metadata": {},
   "source": [
    "### Load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b7891",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(trialPath, dataset_name + '_Formatted_Dataset.csv')\n",
    "# Load in the dataset\n",
    "Dataset_df = pd.read_csv(file_name)\n",
    "# For each Data, convert the string back to an array\n",
    "Dataset_df['Data'] = Dataset_df['Data'].apply(lambda x: np.array(json.loads(x)))\n",
    "Dataset_df.shape\n",
    "Dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd1a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the time of the sample\n",
    "sample_time = Dataset_df.iloc[0,5]\n",
    "print(float(sample_time))\n",
    "# convert the creation time to a readable format\n",
    "sample_time_str = time.strftime('%H:%M:%S', time.localtime(sample_time))\n",
    "print(f'The sample was taken at {sample_time_str}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70a800bc",
   "metadata": {},
   "source": [
    "## Visualization Trials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "942371e9",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0946b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that I have everything compiled into a dataframe, I can now split the data\n",
    "\n",
    "# Split the whole dataste into data into two classes\n",
    "allNormal_df = Dataset_df[Dataset_df['Label (numeric)'] == 0]\n",
    "allCancer_df = Dataset_df[Dataset_df['Label (numeric)'] == 1]\n",
    "\n",
    "# Get just data which sampleID contains 1_front\n",
    "sample1Normal_df = allNormal_df[allNormal_df['SampleID'].str.contains('1')]\n",
    "sample1Cancer_df = allCancer_df[allCancer_df['SampleID'].str.contains('1')]\n",
    "\n",
    "data_0_df = allNormal_df\n",
    "#print the shape of the data\n",
    "print(data_0_df.shape)\n",
    "data_1_df = allCancer_df\n",
    "#print the shape of the data\n",
    "print(data_1_df.shape)\n",
    "\n",
    "# Extract the data from the dataframe\n",
    "data_0 = np.array(data_0_df['Data'].tolist())\n",
    "labels0 = np.array(data_0_df['Label (numeric)'].tolist())\n",
    "data_1 = np.array(data_1_df['Data'].tolist())\n",
    "labels1 = np.array(data_1_df['Label (numeric)'].tolist())\n",
    "\n",
    "# labels1 = data_1_df['Label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e7206",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Normal data shape: ',data_0.shape)\n",
    "print('Cancer data shape: ',data_1.shape)\n",
    "# print the labels shape\n",
    "print('Normal labels: ',labels0.shape)\n",
    "print('Cancer labels: ',labels1.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3d0d0a9",
   "metadata": {},
   "source": [
    "#### Load in the broadband transfer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSpectrum(path, col_name=None,start_index=774,end_index=-1,sep=';'):\n",
    "#     df = pd.read_csv(path + name,sep=';',engine='python')\n",
    "    df = pd.read_csv(path,sep=sep,engine='python')\n",
    "#     print(df)\n",
    "    if not(col_name == None):\n",
    "        df[col_name] = df.index\n",
    "    data = df[start_index:end_index]\n",
    "#     print(data)\n",
    "    data_arr = data.to_numpy()\n",
    "    data_arr = np.array(data_arr,dtype='float')\n",
    "    return data_arr\n",
    "\n",
    "# LOAD IN BASELINES\n",
    "\n",
    "dataPath = os.getcwd()\n",
    "folderName = 'March2022_raw_data'\n",
    "file_name = 'SLS201L_Spectrum_reformatted.csv'\n",
    "dataPath_BrOut = os.path.join(dataPath,\"data\",folderName,file_name)\n",
    "print(dataPath_BrOut)\n",
    "baseline_BrOut_raw = loadSpectrum(dataPath_BrOut, 'Wavelength', start_index=10, end_index=675, sep=',')\n",
    "# Interpolate such that the downloaded spectrum has the same values of the data\n",
    "x = baseline_BrOut_raw[:,0]\n",
    "y = baseline_BrOut_raw[:,1]\n",
    "f = interpolate.interp1d(x,y)\n",
    "xnew = data_0[0,:,0]\n",
    "ynew = f(xnew)\n",
    "baseline_BrOut = np.transpose(np.array([xnew,ynew]))\n",
    "print(baseline_BrOut.shape)\n",
    "print('Data shape', data_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c537e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(data_0[0,:,0],data_0[0,:,1])\n",
    "# plt.plot(data_1[0,:,0],data_1[0,:,1])\n",
    "# data_0.shape\n",
    "# print(data_0[0,0,0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7c74cf1",
   "metadata": {},
   "source": [
    "### Display the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23590877",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Displaying all of the spectra to visually inspect results\n",
    "\n",
    "# This should be in GUI with all the inputs as parameters\n",
    "wavelength_start = data_0[0,0,0]\n",
    "wavelength_end = data_0[0,-1,0]\n",
    "\n",
    "w = np.linspace(wavelength_start,wavelength_end,len(data_0[1]))\n",
    "# # # Display an example of data_0\n",
    "# GUI.plotSpectra(xdata=data_0[0,:,0],ydata=data_0[0,:,1],xlab='Wavelength(nm)',ylab='Reflected Intensity',\n",
    "#                 title='Unprocessed data_0 Spectrum')\n",
    "# # Display an example of data_1\n",
    "# GUI.plotSpectra(xdata=data_1[0,:,0], ydata=data_1[0,:,1],xlab='Wavelength(nm)',ylab='Reflected Intensity',\n",
    "#                 title='Unprocessed data_1 Spectrum' )\n",
    "# Display an example of broadband output\n",
    "# GUI.plotSpectra(xdata=baseline_BrOut[:,0],ydata=baseline_BrOut[:,1],xlab=\"Wavelength[nm]\", ylab='Incident Intensity',title='baseline_BrOut')\n",
    "\n",
    "# Plot the baseline_BrOut using plotly\n",
    "import plotly.graph_objects as go\n",
    "wavelengths = baseline_BrOut[:,0]\n",
    "intensities = baseline_BrOut[:,1]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=wavelengths, y=intensities, mode='lines', line=dict(color='blue')))\n",
    "# Customize the plot layout\n",
    "fig.update_layout(xaxis_title='Wavelength (nm)',\n",
    "                  yaxis_title='Normalize Intensity',\n",
    "                  showlegend=False,\n",
    "                  plot_bgcolor='white',\n",
    "                  font=dict(size=16),\n",
    "                  width=800,  # Set the width of the figure in pixels\n",
    "                  height=600  # Set the height of the figure in pixels\n",
    "                  )\n",
    "fig.update_xaxes(\n",
    "    ticks='outside',\n",
    "    showline=True,\n",
    "    linecolor='black',\n",
    "    gridcolor='lightgrey'\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    ticks='outside',\n",
    "    showline=True,\n",
    "    linecolor='black',\n",
    "    gridcolor='lightgrey'\n",
    ")\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c5d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot all the spectra\n",
    "def plotAll(data,title='', xtitle='', ytitle=''):\n",
    "    plt.figure()\n",
    "    for i in range(len(data)):\n",
    "        plt.plot(data[i,:,0],data[i,:,1])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xtitle)\n",
    "    plt.ylabel(ytitle)\n",
    "\n",
    "\n",
    "def plotWColourMap(data, title, xlabel, ylabel, step=1):\n",
    "    \"\"\"\n",
    "    Plot all samples of class1 on a single figure using a colour map to denote chronological order.\n",
    "\n",
    "    Args:\n",
    "    - data (ndarray): a 3D array of shape (num_samples, num_wavelengths, 2) containing the spectral data\n",
    "    - title (str): the title of the plot\n",
    "    - xlabel (str): the label of the x-axis\n",
    "    - ylabel (str): the label of the y-axis\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    # create the colour map\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "\n",
    "    # plot the data with the colour map\n",
    "    plt.figure()\n",
    "    for i in range(len(data)-1):\n",
    "        # plot every third spectra\n",
    "        if i%step == 0:\n",
    "            plt.scatter(data[i,:,0], data[i,:,1], s=0.1, c=cmap(i/len(data)))\n",
    "\n",
    "    # add a color bar\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    sm.set_array([])\n",
    "    plt.colorbar(sm)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all samples of class0 on a single figure without using plotSpectra\n",
    "plotAll(data_0,'All normal Spectra','Wavelength[nm]','Reflected Intensity')\n",
    "\n",
    "# Plot all samples of class1 on a single figure without using plotSpectra\n",
    "plotAll(data_1,'All cancer Spectra','Wavelength[nm]','Reflected Intensity')\n",
    "\n",
    "# GUI.plotSpectra(xdata=data_1[0,:,0], ydata=data_1[0,:,1],xlab='Wavelength(nm)',ylab='Reflected Intensity',\n",
    "#                 title='Unprocessed data_1 Spectrum' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT the location of the max point for each spectrum\n",
    "def plotMax(data, title, xlabel, ylabel):\n",
    "    \"\"\"\n",
    "    Plot the location of the max point for each spectrum.\n",
    "\n",
    "    Args:\n",
    "    - data (ndarray): a 3D array of shape (num_samples, num_wavelengths, 2) containing the spectral data\n",
    "    - title (str): the title of the plot\n",
    "    - xlabel (str): the label of the x-axis\n",
    "    - ylabel (str): the label of the y-axis\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    # find the max point for each spectrum\n",
    "    max_points_indices = np.argmax(data[:,:,1], axis=1)\n",
    "    # Find the corresponding wavelength\n",
    "    max_points = data[:,max_points_indices,0]\n",
    "    # plot the max points\n",
    "    SampleNumber = range(len(data))\n",
    "    plt.figure()\n",
    "    plt.scatter(max_points[0],range(len(data)))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    # change the range of the x-axis\n",
    "    plt.xlim(400, 1000)\n",
    "\n",
    "# concatinate the data_0 and data_1\n",
    "d = np.concatenate((data_0, data_1), axis=0)\n",
    "# Plot the location of the max point for each spectrum of class0\n",
    "plotMax(d, 'Max point for all spectra', 'Wavelength index', 'Spectrum number')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_0_save = data_0.copy()\n",
    "# data_1_save = data_1.copy()\n",
    "\n",
    "# data_0 = data_0_save.copy()\n",
    "# data_1 = data_1_save.copy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7dea0d9f",
   "metadata": {},
   "source": [
    "### Ambient light removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load in the ambient light and the raw signal\n",
    "def formatAmbientLight(dataPath,start_index=790,end_index=-1,sep=','):\n",
    "    '''\n",
    "    Function: loadDataset\n",
    "    ---------------------\n",
    "    This function loads in the measured ambient light from a given dataPath, it \n",
    "    The dataPath leads to a folder. Open the folders containing _AmbientLight in the name. Load and format the data contained.\n",
    "    '''\n",
    "    Signal = []\n",
    "    AmbientLight = []\n",
    "    # print(\"Loading in: \", dataPath)\n",
    "    for name in os.listdir(dataPath):\n",
    "        # print(name)\n",
    "        df = pd.read_csv(os.path.join(dataPath,name), sep=sep,engine='python', header=None)\n",
    "        df = df.iloc[:, start_index:]\n",
    "        data_arr = df.to_numpy()\n",
    "        spectrum_arr = np.mean(data_arr[1:, 1:],axis=0)\n",
    "        wavelength_arr = data_arr[0, 1:]\n",
    "        data_arr = np.concatenate((wavelength_arr.reshape(-1,1), spectrum_arr.reshape(-1,1)), axis=1)\n",
    "        if name.endswith('_AmbientLight.csv'):\n",
    "            AmbientLight.append(data_arr)\n",
    "        else:\n",
    "            Signal.append(data_arr)\n",
    "    Signal = np.array(Signal,dtype='float')\n",
    "    AmbientLight = np.array(AmbientLight,dtype='float')\n",
    "    # Average the singal and ambient light\n",
    "    Signal = np.mean(Signal,axis=0)\n",
    "    AmbientLight = np.mean(AmbientLight,axis=0)\n",
    "    return Signal, AmbientLight\n",
    "\n",
    "# Step 2: Calculate the ratio of the signal to the ambient light\n",
    "def calcAmbientRatio(signal):\n",
    "    # Find the peak ambient light value in the range of 1110 to 1130\n",
    "    ambient_peak_value = np.max(signal[1110:1130,1])\n",
    "    # Find the peak signal value in the range of 1200 to 1800\n",
    "    signal_peak_value = np.max(signal[1200:1800,1])\n",
    "    # calculate the ratio of the signal to the ambient light\n",
    "    ratio = signal_peak_value/ambient_peak_value\n",
    "    return ratio\n",
    "\n",
    "# Step 3: Use the baseline ratio and the measured ambient light signal to eliminate the ambient light from the remaining signals\n",
    "def removeAmbientLight1(data,ambient_baseline,ratio_baseline):\n",
    "    # For each signal in dataset, calulate the ratio of the signal to the ambient light\n",
    "    ratio_data = calcAmbientRatio(data)\n",
    "    # print(\"ratio_data\",ratio_data)\n",
    "    # compare to the baseline ratio\n",
    "    ratio_comparison = (ratio_baseline/ratio_data)\n",
    "    # scale the ambient light by the comparison ratio\n",
    "    scaled_ambient = ambient_baseline.copy()\n",
    "    scaled_ambient[:,1] = scaled_ambient[:,1]/ratio_comparison\n",
    "    # subtract the scaled ambient light from the signal\n",
    "    data[:,1] = data[:,1] - scaled_ambient[:,1]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e11000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Calculate the baseline ambient light ratio for each sample \n",
    "'''\n",
    "dataPath = \"C:/Users/David/OneDrive - Queen's University/1 Graduate Studies/1 Thesis Research/KidneyData_march3/March3_KidneyCollectionWithDrRen/Mar03\"\n",
    "# print(\"Loading in: \", dataPath)\n",
    "# find all of the sample folders\n",
    "sampleNameList = [f for f in os.listdir(dataPath) if f.startswith('Patient')]\n",
    "# print(\"List of samples\", sampleNameList)\n",
    "baseline_ratios = []\n",
    "baseline_ambient = []\n",
    "trialName = []\n",
    "for sampleName in sampleNameList:\n",
    "    # append the sample name to the dataPath\n",
    "    samplePath = os.path.join(dataPath,sampleName)\n",
    "    # print(\"Loading in: \", samplePath)\n",
    "    # find all of the folders that end in _AmbientLight\n",
    "    ambientLightNameList = [f for f in os.listdir(samplePath) if f.endswith('AmbientLight')]\n",
    "    # print(ambientLightNameList) \n",
    "    for name in ambientLightNameList:\n",
    "        path = os.path.join(samplePath,name)\n",
    "        print(\"Loading in: \", sampleName + ' ' + name)\n",
    "        # Step 1: Load in the ambient light and the raw signal\n",
    "        # path = \"C:/Users/David/OneDrive - Queen's University/1 Graduate Studies/1 Thesis Research/KidneyData_march3/March3_KidneyCollectionWithDrRen/Mar03/PatientA_Sample3_front/Cancer_AmbientLight\"\n",
    "        signal, ambientLight = formatAmbientLight(path)\n",
    "\n",
    "        # Step 2: Calculate the ratio of the signal to the ambient light\n",
    "        ratio_baseline = calcAmbientRatio(signal)\n",
    "        print(\"baseline ratio\",ratio_baseline)\n",
    "\n",
    "        # Step 3: Use the baseline ratio and the measured ambient light signal to eliminate the ambient light from the remaining signals\n",
    "        ambient_baseline = ambientLight.copy()\n",
    "        ratio_baseline\n",
    "        baseline_ratios.append(ratio_baseline)\n",
    "        baseline_ambient.append(ambient_baseline)\n",
    "\n",
    "        # The information I need is the sampleID and the class\n",
    "        sampleID = sampleName.split('_')[1] + '_' +sampleName.split('_')[2]\n",
    "        # extract cancer or normal from the name\n",
    "        className = name.split('_')[0]\n",
    "        # if string className contains cancer, then label = 1\n",
    "        print(className)\n",
    "        if className == 'Cancer':\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        \n",
    "        trialName.append(str(label) + '-' + sampleID)\n",
    "        # break\n",
    "    # break\n",
    "print(baseline_ratios)\n",
    "print(trialName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2fec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I now have the baseline ratios and the baseline ambient light for each sample\n",
    "# I can now use this to remove the ambient light from the data\n",
    "# Create a dataframe to store the baseline ratios and the baseline ambient light\n",
    "baseline_df = pd.DataFrame(columns=['SampleID', 'Label (numeric)', 'Baseline Ambient Light', 'Baseline Ratio'])\n",
    "for i in range(len(baseline_ratios)):\n",
    "    # Split the trial name into the class and the sampleID\n",
    "    # print(trialName[i])\n",
    "    sampleID = trialName[i].split('-')[1]\n",
    "    # print(sampleID)\n",
    "    className = trialName[i].split('-')[0]\n",
    "    # print(className)\n",
    "    new_row = {'SampleID':sampleID,\n",
    "               'Label (numeric)':className,\n",
    "               'Baseline Ambient Light':baseline_ambient[i],\n",
    "               'Baseline Ratio':baseline_ratios[i]\n",
    "    }\n",
    "    baseline_df = pd.concat([baseline_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "baseline_df['Baseline Ambient Light'] = baseline_df['Baseline Ambient Light'].apply(lambda x: json.dumps(x.tolist()))\n",
    "file_name = os.path.join(trialPath, 'ambient_baseline_and_ratios.csv')\n",
    "baseline_df.to_csv(file_name, index=False)\n",
    "file_name = os.path.join(trialPath, 'ambient_baseline_and_ratios.csv')\n",
    "\n",
    "baseline_df = 0\n",
    "# Load in the ambient light baselines\n",
    "baseline_df = pd.read_csv(file_name)\n",
    "# For each Data, convert the string back to an array\n",
    "baseline_df['Baseline Ambient Light'] = baseline_df['Baseline Ambient Light'].apply(lambda x: np.array(json.loads(x)))\n",
    "\n",
    "\n",
    "\n",
    "# Loop through the dataset and remove the ambient light\n",
    "baseline_df\n",
    "Dataset_df\n",
    "# For each sample ID and label in the baseline_df\n",
    "# find unique sampleIDs\n",
    "# sampleIDs = baseline_df['sampleID'].unique()\n",
    "# print(sampleIDs)\n",
    "# for sampleID in sampleIDs:\n",
    "#     # print(sampleID)\n",
    "#     labels = baseline_df['Label'].unique()\n",
    "#     print(labels)\n",
    "\n",
    "# Attempt 2\n",
    "# Loop through the dataset and remove the ambient light\n",
    "# Given a data, find the corresponding baseline ambient light and the baseline ratio\n",
    "# for each sample in the dataset\n",
    "def removeAmbientLight_method2(Dataset_df, baseline_df):\n",
    "    Dataset = []\n",
    "    for i in range(Dataset_df.shape[0]):\n",
    "        # Get the data\n",
    "        data = np.array(Dataset_df['Data'].tolist())[i,:,:]\n",
    "        # Get the sampleID\n",
    "        sampleID = Dataset_df['SampleID'][i]\n",
    "        # Get the label\n",
    "        label = Dataset_df['Label (numeric)'][i]\n",
    "        # print(label)\n",
    "        # Find the corresponding baseline ambient light and the baseline ratio\n",
    "        # Get the row of baseline_df where baseline_df['sampleID'] == sampleID and \n",
    "        baseline_sample_row = baseline_df.loc[(baseline_df['SampleID'] == sampleID)]\n",
    "        # Get the row of baseline_df where baseline_df['Label (numeric)'] == Label (numeric)\n",
    "        # print(baseline_sample_row['Label (numeric)']=='1')\n",
    "        baseline_row = baseline_sample_row.loc[(baseline_sample_row['Label (numeric)'] == str(label))]\n",
    "        # print(baseline_row)\n",
    "        # if baseline_row is empty give an error\n",
    "        if baseline_row.empty:\n",
    "            print(\"Error: baseline_row is empty\")\n",
    "            break\n",
    "        \n",
    "        # Get the baseline ambient light\n",
    "        ambient_baseline = np.array(baseline_row['Baseline Ambient Light'].tolist())[0,:,:]\n",
    "        # Get the baseline ratio\n",
    "        ratio_baseline = np.array(baseline_row['Baseline Ratio'].tolist())[0]\n",
    "\n",
    "        # Remove ambient light peak\n",
    "        data = removeAmbientLight1(data,ambient_baseline,ratio_baseline)\n",
    "\n",
    "        Dataset.append(data)\n",
    "\n",
    "        # Normalize the data\n",
    "        # data = process.normalize(data)\n",
    "        # Display the data\n",
    "        # GUI.plotSpectra(xdata=data[:,0],ydata=data[:,1],xlab='Wavelength(nm)',ylab='Reflected Intensity', title='Signal Spectrum after subtracting ambient light')\n",
    "    # Return the dataset as an array\n",
    "    Dataset = np.array(Dataset,dtype='float')\n",
    "    return Dataset\n",
    "\n",
    "# Call the function to remove the ambient light\n",
    "# data = removeAmbientLight_method2(Dataset_df,baseline_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporayily remove the peak caused by ambient light\n",
    "data_0_new = data_0.copy()\n",
    "data_1_new = data_1.copy()\n",
    "# Set the spectra to zero between 600 and 650 nm\n",
    "# This is to remove the background noise using no functions\n",
    "def removeAmbientLight_method1(data):\n",
    "    start_index = 1110\n",
    "    width = 20\n",
    "    end_index = start_index + width\n",
    "    for i in range(len(data)):\n",
    "        data[i,start_index:end_index,1] = 0\n",
    "    return data\n",
    "\n",
    "data_0_new = removeAmbientLight_method1(data_0_new)\n",
    "data_1_new = removeAmbientLight_method1(data_1_new)\n",
    "\n",
    "# concatinate the data_0 and data_1\n",
    "d = np.concatenate((data_0_new, data_1_new), axis=0)\n",
    "# Plot the location of the max point for each spectrum of class0\n",
    "plotMax(d, 'Max point for all spectra with ambient peak removed', 'Wavelength index', 'Spectrum number')\n",
    "\n",
    "# Use the refleccted ambient light recorded before the experiment to estimate the ambient light from the rest of the experiment.\n",
    "\n",
    "# Load in the ambient light"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab875077",
   "metadata": {},
   "source": [
    "### Preprocessing of the data\n",
    "* Normalize so peak is 1\n",
    "* Crop to 360nm to 1024nm\n",
    "* Divide by the broadband output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da308e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "data_0_norm = data_0_new.copy()\n",
    "data_1_norm = data_1_new.copy()\n",
    "data_0_norm = process.normalize(data_0_norm)\n",
    "data_1_norm = process.normalize(data_1_norm)\n",
    "\n",
    "# Plot them again\n",
    "# Plot all samples of class0 on a single figure without using plotSpectra\n",
    "plt.figure()\n",
    "for i in range(len(data_0_norm)):\n",
    "    plt.scatter(data_0_norm[i,:,0] ,data_0_norm[i,:,1],s=0.1)\n",
    "plt.title('All normal Spectra (MinMax Normalized))')\n",
    "plt.xlabel('Wavelength[nm]')\n",
    "plt.ylabel('Reflected Intensity')\n",
    "\n",
    "# Plot all samples of class1 on a single figure without using plotSpectra\n",
    "plt.figure()\n",
    "for i in range(len(data_1_norm)):\n",
    "    plt.scatter(data_1_norm[i,:,0],data_1_norm[i,:,1],s=0.5)\n",
    "plt.title('All cancer Spectra (MinMax Normalized)')\n",
    "plt.xlabel('Wavelength[nm]')\n",
    "plt.ylabel('Reflected Intensity')\n",
    "\n",
    "# # plot one cancer and one normal spectrum on a scatter plot\n",
    "# plt.figure()\n",
    "# plt.scatter(data_0_norm[0,:,0],data_0_norm[0,:,1],s=0.5)\n",
    "# plt.scatter(data_1_norm[0,:,0],data_1_norm[0,:,1],s=0.5)\n",
    "# plt.title('One cancer and one normal spectrum [MinMax Normalized]')\n",
    "# plt.xlabel('Wavelength[nm]')\n",
    "# plt.ylabel('Reflected Intensity')\n",
    "# plt.legend(['Normal','Cancer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b670be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot each spectra using a color map to show how the spectra change over time\n",
    "# # This is to see if there is any pattern in the spectra\n",
    "# # use scatter plot to show the data points\n",
    "\n",
    "# # create the colour map\n",
    "# cmap = plt.get_cmap('viridis')\n",
    "\n",
    "# # plot the data with the colour map\n",
    "# plt.figure()\n",
    "# for i in range(len(data_1_norm)-1):\n",
    "#     plt.scatter(data_1_norm[i,:,0],data_1_norm[i,:,1],s=0.1,c=cmap(i/len(data_1_norm)))\n",
    "\n",
    "# # add a color bar\n",
    "# sm = plt.cm.ScalarMappable(cmap=cmap)\n",
    "# sm.set_array([])\n",
    "# plt.colorbar(sm)\n",
    "\n",
    "# plt.title('All normal Spectra (MinMax Normalized)')\n",
    "# plt.xlabel('Wavelength[nm]')\n",
    "# plt.ylabel('Reflected Intensity')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e46916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in the baseline \n",
    "baseline = baseline_BrOut # --------------------------------- flag\n",
    "baseline = process.normalize(baseline)[:,1]\n",
    "\n",
    "data_0_norm = data_0_new[:,280:,:].copy()\n",
    "data_1_norm = data_1_new[:,280:,:].copy()\n",
    "baseline = baseline[280:].copy()\n",
    "tFunc = baseline\n",
    "\n",
    "def divTfuc(inputData,tFunc):    \n",
    "    outputData = inputData.copy()\n",
    "    # For each spectra\n",
    "    for i in range (inputData[:,:,1].shape[0]):\n",
    "        data = inputData[i,:,1]\n",
    "        # Divide by the baseline transfer function\n",
    "        outputData[i,:,1] = data / tFunc \n",
    "    outputData = process.normalize(outputData)\n",
    "    return outputData\n",
    "# call the function\n",
    "data_0_norm_T = divTfuc(data_0_norm,tFunc)\n",
    "data_1_norm_T = divTfuc(data_1_norm,tFunc)\n",
    "# Display the arguemtns and output\n",
    "freq = data_0_norm[0,:,0]\n",
    "\n",
    "# Plot all samples of class0 on a single figure without using plotSpectra\n",
    "# assuming data_1_norm_T is the input data array\n",
    "plotWColourMap(data_0_norm_T, 'All Normal Spectra (Tfunc Norm)', 'Wavelength[nm]', 'Reflected Intensity', )\n",
    "\n",
    "# Plot all samples of class1 on a single figure using a colour map to denote chronilogical order\n",
    "# assuming data_1_norm_T is the input data array\n",
    "plotWColourMap(data_1_norm_T, 'All cancer Spectra (Tfunc Norm)', 'Wavelength[nm]', 'Reflected Intensity')\n",
    "\n",
    "\n",
    "# plot the average of the cancer spectra and the average of the normal spectra on a scatter plot\n",
    "plt.figure()\n",
    "plt.scatter(data_0_norm_T[:,:,0].mean(axis=0),data_0_norm_T[:,:,1].mean(axis=0),s=0.5)\n",
    "plt.scatter(data_1_norm_T[:,:,0].mean(axis=0),data_1_norm_T[:,:,1].mean(axis=0),s=0.5)\n",
    "plt.title('Average cancer and normal spectra [Divided by broad band]')\n",
    "plt.xlabel('Wavelength[nm]')\n",
    "plt.ylabel('Reflected Intensity')\n",
    "plt.legend(['Normal','Cancer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc29ad1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_0_norm = data_0_norm_T\n",
    "data_1_norm = data_1_norm_T\n",
    "\n",
    "# # Plots of the normalized spectra\n",
    "# GUI.plotSpectra(xdata=data_0_norm[0,:,0],ydata=data_0_norm[0,:,1],xlab='Wavelength(nm)',ylab='Reflected Intensity',\n",
    "#                 title='Normalized data_0 Spectrum' )\n",
    "# GUI.plotSpectra(xdata=data_1_norm[0,:,0],ydata=data_1_norm[0,:,1],xlab='Wavelength(nm)',ylab='Reflected Intensity',\n",
    "#                 title='Normalized data_1 Spectrum' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels1\n",
    "# # reindex the labels\n",
    "# labels1 = labels1.reset_index(drop=True)\n",
    "# # get the index of cancer\n",
    "# # cancer_index = labels1[labels1['label'] == 1].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3c89f22",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d295aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(Dataset_df,ambient_baseline_df, FLAG_AMBIENT_LIGHT,FLAG_CROP, FLAG_NORMALIZE, FLAG_TFUNC):\n",
    "    # Step 1: Remove ambient light peak method 1 or 2\n",
    "    if FLAG_AMBIENT_LIGHT:\n",
    "        if FLAG_AMBIENT_LIGHT == 1:\n",
    "            # print(FLAG_AMBIENT_LIGHT)\n",
    "            data = np.array(Dataset_df['Data'].tolist())\n",
    "            data = removeAmbientLight_method1(data) # Method 1: Remove just the peak of the ambient light\n",
    "        if FLAG_AMBIENT_LIGHT == 2:\n",
    "            # print(FLAG_AMBIENT_LIGHT)\n",
    "            data = removeAmbientLight_method2(Dataset_df,ambient_baseline_df) # Method 2: Estimate the ambient light for each scan\n",
    "    else:\n",
    "        data = np.array(Dataset_df['Data'].tolist())\n",
    "\n",
    "    # Step 2: Crop the data to 420 nm\n",
    "    if FLAG_CROP:\n",
    "        data = data[:,280:,:]\n",
    "\n",
    "    # Step 3: Normalize the data using minimax\n",
    "    if FLAG_NORMALIZE: \n",
    "        data = process.normalize(data)\n",
    "\n",
    "    # Step 4: Divide by the baseline transfer function\n",
    "    if FLAG_TFUNC:\n",
    "        data = divTfuc(data,tFunc)\n",
    "\n",
    "    # Step 5: Feature reduction\n",
    "\n",
    "    processed_data = data.copy()\n",
    "\n",
    "    # Step 6: Turn the processed data into a singe data column in a dataframe\n",
    "    data_df = pd.DataFrame()\n",
    "    for i in range(processed_data.shape[0]):\n",
    "        new_row = {'Data_preprocessed':processed_data[i,:,:]}\n",
    "        data_df = pd.concat([data_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    # Add the data to the dataframe\n",
    "    processedData_df = pd.concat([Dataset_df, data_df], axis=1)\n",
    "    return processedData_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d13263c3",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5203d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCSSING THE DATA\n",
    "\n",
    "# Load in the dataset from file \n",
    "file_name = os.path.join(trialPath, dataset_name + '_Formatted_Dataset.csv')\n",
    "# Load in the dataset\n",
    "Dataset_df = pd.read_csv(file_name)\n",
    "# For each Data, convert the string back to an array\n",
    "Dataset_df['Data'] = Dataset_df['Data'].apply(lambda x: np.array(json.loads(x)))\n",
    "\n",
    "# Load in the ambient light baseline dataframe\n",
    "ambient_baseline_df = baseline_df.copy()\n",
    "\n",
    "# Preprocessing flags\n",
    "FLAG_AMBIENT_LIGHT = 1 # Method 1, Method2, None\n",
    "FLAG_CROP = True\n",
    "FLAG_NORMALIZE = True\n",
    "FLAG_TFUNC = True\n",
    "FLAG_FEATURE_REDUCTION = False\n",
    "\n",
    "processedData_df = preprocessing_pipeline(Dataset_df,ambient_baseline_df, FLAG_AMBIENT_LIGHT,FLAG_CROP, FLAG_NORMALIZE, FLAG_TFUNC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60174bae",
   "metadata": {},
   "source": [
    "## 3D PCA trials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fe25e27",
   "metadata": {},
   "source": [
    "### PCA fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01156852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Get the data from the dataframe\n",
    "data = np.array(processedData_df['Data_preprocessed'].tolist())[:,:,1]\n",
    "# Create the PCA object\n",
    "pca = PCA(n_components=3)\n",
    "# Fit the PCA object to the data\n",
    "pca.fit(data)\n",
    "# Transform the data\n",
    "data_pca = pca.fit_transform(data)\n",
    "# Create a dataframe with the PCA data\n",
    "data_pca_df = pd.DataFrame(data_pca, columns=['PCA1','PCA2','PCA3'])\n",
    "# Add the PCA data to the dataframe\n",
    "data_pca_df = pd.concat([processedData_df, data_pca_df], axis=1)\n",
    "# data_pca_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "474e9935",
   "metadata": {},
   "source": [
    "### PCA - Class separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc3a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "bigPlot = False\n",
    "if bigPlot:\n",
    "    figSize = (800,800)\n",
    "else:\n",
    "    figSize = (400,400)\n",
    "    \n",
    "# Create a 3D PCA plot of cancer vs normal\n",
    "fig = px.scatter_3d(data_pca_df, x='PCA1', y='PCA2', z='PCA3',title='3D PCA: Cancer vs Normal', color='Label', opacity=0.8, color_discrete_sequence=['orange','blue','orange','orange','orange'])\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(width=figSize[0], height=figSize[1],title_x=0.5, title_y=0.8,legend=dict(x=0.8, y=0.8))\n",
    "fig.show()\n",
    "# Create a 3D plot of all classes in the dataset\n",
    "# color_discrete_sequence=['yellow','blue','orange','pink','red']\n",
    "fig = px.scatter_3d(data_pca_df, x='PCA1', y='PCA2', z='PCA3',title='3D PCA: all classes in the dataset', color='Label', opacity=0.8, color_discrete_sequence=['yellow','blue','orange','pink','red'])\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(width=figSize[0], height=figSize[1],title_x=0.5, title_y=0.8,legend=dict(x=0.8, y=0.8))\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe2a552a",
   "metadata": {},
   "source": [
    "### PCA - Sample separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e908b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigPlot = False\n",
    "if bigPlot:\n",
    "    figSize = (800,800)\n",
    "else:\n",
    "    figSize = (400,400)\n",
    "# 3D PCA: Sample1 vs Sample2 vs Sample3\n",
    "fig = px.scatter_3d(data_pca_df, x='PCA1', y='PCA2', z='PCA3',title='3D PCA: Sample1 vs Sample2 vs Sample3', color='SampleID', opacity=0.8, color_discrete_sequence=['orange','orange','red','red','pink','pink'])\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(width=figSize[0], height=figSize[1],title_x=0.5, title_y=0.8,legend=dict(x=0.8, y=0.8))\n",
    "fig.show()\n",
    "\n",
    "# 3D PCA: plot pf cancer vs normal for sample 1 and 3\n",
    "data_1_3_df = data_pca_df[data_pca_df['SampleID'].str.contains('Sample1|Sample3')]\n",
    "fig = px.scatter_3d(data_1_3_df, x='PCA1', y='PCA2', z='PCA3',title='3D PCA: plot pf cancer vs normal for sample 1 and 3', color='Label', opacity=0.8, color_discrete_sequence=['orange','blue','orange','orange','orange','orange'])\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(width=figSize[0], height=figSize[1],title_x=0.5, title_y=0.8,legend=dict(x=0.8, y=0.8))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd86ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigPlot = False\n",
    "if bigPlot:\n",
    "    figSize = (800,800)\n",
    "else:\n",
    "    figSize = (400,400)\n",
    "# 3D PCA: plot pf cancer vs normal for sample 1 and 3\n",
    "# get the sample ID which contain Sample1 and Sample3\n",
    "data_1_3_df = data_pca_df[data_pca_df['SampleID'].str.contains('Sample1|Sample3')]\n",
    "# print size of data_1_3_df\n",
    "print('Size of data_1_3_df: ',data_1_3_df.shape)\n",
    "fig = px.scatter_3d(data_pca_df, x='PCA1', y='PCA2', z='PCA3',title='3D PCA: all classes in the dataset', color='Label', opacity=0.8, color_discrete_sequence=['yellow','blue','orange','pink','red'])\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "\n",
    "fig.update_layout(width=figSize[0], height=figSize[1],title_x=0.5, title_y=0.8,legend=dict(x=0.8, y=0.8))\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4482b09",
   "metadata": {},
   "source": [
    "### Combination graph - class and sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09648dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "bigPlot = True\n",
    "if bigPlot:\n",
    "    figSize = (800,800)\n",
    "else:\n",
    "    figSize = (400,400)\n",
    "# 3D PCA: Sample1 vs Sample2 vs Sample3\n",
    "''' \n",
    "Combine the information from the graphs above: \n",
    "Create a 3D plot which contains all the samples colour coded by sample number\n",
    "also encode the classes in terms of shapes\n",
    "    For example:\n",
    "    Sample1_back (cancer):  light blue circle-open\n",
    "    Sample1_back (normal):  light blue cross\n",
    "    Sample1_front (cancer): blue circle-open\n",
    "    Sample1_front (normal): blue cross\n",
    "    Sample2_back (cancer):  light red circle-open\n",
    "    Sample2_back (normal):  light red cross\n",
    "    ...\n",
    "The main dataframe has the following columns:\n",
    "    SampleID. Contains the sample ID\n",
    "    Label. Contains the class of the sample\n",
    "    Label (numeric). Contains the class of the sample encoded as a number\n",
    "    PCA1. Contains the PCA1 value\n",
    "    PCA2. Contains the PCA2 value\n",
    "    PCA3. Contains the PCA3 value\n",
    "'''\n",
    "# Create a 3D plot\n",
    "labels = np.unique(data_pca_df['SampleID'])\n",
    "# change the colours to be Blue, slightly ligher blue, red, slightly ligher red, green, lighter green\n",
    "colours = ['#0000FF', '#99CCFF', '#FF0000', '#FF9999', '#008000', '#99CC99']\n",
    "shapes = ['circle-open','cross']\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "for label, colour in zip(labels, colours):\n",
    "    df = data_pca_df[data_pca_df['SampleID'] == label]\n",
    "    marker_shapes = [shapes[int(x)] for x in df['Label (numeric)']]\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=df['PCA1'], \n",
    "        y=df['PCA2'], \n",
    "        z=df['PCA3'], \n",
    "        mode='markers', \n",
    "        marker=dict(size=5, color=colour, symbol=marker_shapes),\n",
    "        name=label\n",
    "    ))\n",
    "\n",
    "# combine the update_layout into a single statement\n",
    "fig.update_layout(\n",
    "    # change aspects of the graph\n",
    "    scene=dict(\n",
    "        xaxis_title='PCA1', yaxis_title='PCA2', zaxis_title='PCA3'\n",
    "        # ,xaxis=dict(range=[-5, 5]), \n",
    "        # yaxis=dict(range=[-5, 5]), \n",
    "        # zaxis=dict(range=[-5, 5])\n",
    "        ),\n",
    "    width= figSize[0],\n",
    "    height= figSize[1],\n",
    "    title='3D PCA: Combination graph (Class: Shape, SampleID: Colour)',\n",
    "    # Change title and legend position\n",
    "    title_x=0.5, title_y= 0.8,\n",
    "    legend=dict(x=0.8, y=0.8)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb95c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigPlot = True\n",
    "if bigPlot:\n",
    "    figSize = (800,800)\n",
    "else:\n",
    "    figSize = (400,400)\n",
    "''' \n",
    "Combine the information from the graphs above: \n",
    "Create a 3D plot which contains all the samples colour coded by sample number\n",
    "also encode the classes in terms of shapes\n",
    "    For example:\n",
    "    Sample1_back (cancer):  blue circle-open\n",
    "    Sample1_back (normal):  orange blue circle-open\n",
    "    Sample1_front (cancer): blue circle\n",
    "    Sample1_front (normal): orange circle\n",
    "    Sample2_back (cancer):  blue red square-open\n",
    "    Sample2_back (normal):  orange red square-open\n",
    "    ...\n",
    "The main dataframe has the following columns:\n",
    "    SampleID. Contains the sample ID\n",
    "    Label. Contains the class of the sample\n",
    "    Label (numeric). Contains the class of the sample encoded as a number\n",
    "    PCA1. Contains the PCA1 value\n",
    "    PCA2. Contains the PCA2 value\n",
    "    PCA3. Contains the PCA3 value\n",
    "'''\n",
    "# Create a 3D plot\n",
    "labels = np.unique(data_pca_df['SampleID'])\n",
    "\n",
    "# Change the colors to be blue for label 0 and red for label 1\n",
    "colors = {0: 'blue', 1: 'orange'}\n",
    "# Change the shapes to be different for each sample ID\n",
    "shapes = ['circle','circle-open','square','square-open', 'diamond','diamond-open']\n",
    "\n",
    "fig = go.Figure()\n",
    "count = -1\n",
    "for label in labels:\n",
    "    count = count + 1\n",
    "    df = data_pca_df[data_pca_df['SampleID'] == label]\n",
    "    marker_colors = [colors[x] for x in df['Label (numeric)']]\n",
    "    # This marker appears to be chosen randomly\n",
    "    # marker_symbols = [shapes[i % len(shapes)] for i in range(len(df))]\n",
    "    # Here is the line to choose the marker based on the sample ID\n",
    "    marker_symbols = [shapes[count] for i in range(len(df))]\n",
    "\n",
    "\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=df['PCA1'], \n",
    "        y=df['PCA2'], \n",
    "        z=df['PCA3'], \n",
    "        mode='markers', \n",
    "        marker=dict(size=5, color=marker_colors, symbol=marker_symbols),\n",
    "        name=label\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(xaxis_title='PCA1', yaxis_title='PCA2', zaxis_title='PCA3'), \n",
    "    width=figSize[0], \n",
    "    height=figSize[1],\n",
    "    title='3D PCA: Combination graph (Class: Colour, SampleID: Shape)'\n",
    ")\n",
    "\n",
    "# move the title down\n",
    "fig.update_layout(title_x=0.5, title_y=0.8)\n",
    "# move the legend down\n",
    "fig.update_layout(legend=dict(x=0.8, y=0.8))\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23ad19db",
   "metadata": {},
   "source": [
    "### PCA over time: Analyzing the shift of the cancer over time for each sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "bigPlot = False\n",
    "if bigPlot:\n",
    "    figSize = (800,800)\n",
    "else:\n",
    "    figSize = (400,400)\n",
    "# Analyzing the shift of the cancer over time for each sample\n",
    "sampleLabel = 1 # 0 for Sample1_back, 1 for Sample1_front, 2 for Sample2_back, 3 for Sample2_front, 4 for Sample3_back, 5 for Sample3_front\n",
    "histopathologyLabel = 1 # 0 for normal, 1 for cancer\n",
    "#if histopathologylabel is 0, histopathologyLabelText is 'normal', else it is normal \n",
    "if histopathologyLabel ==0:\n",
    "    histopath = 'Normal'\n",
    "else:\n",
    "    histopath = 'Cancer'\n",
    "\n",
    "# Get the desired histopathology from dataframe\n",
    "data_desiredPathology_df = data_pca_df[data_pca_df['Label (numeric)'] == histopathologyLabel]\n",
    "# Get uniqiue ids from the SampleID column\n",
    "unique_ids = data_pca_df['SampleID'].unique()\n",
    "\n",
    "\n",
    "# Get desired sample from dataset\n",
    "data_desiredPathology_df = data_desiredPathology_df[data_desiredPathology_df['SampleID'] == unique_ids[sampleLabel]]\n",
    "\n",
    "# fit a pca model to the cancer samples\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(data_desiredPathology_df[['PCA1','PCA2','PCA3']])\n",
    "# transform the data\n",
    "data_desiredPathology_df[['PCA1','PCA2','PCA3']] = pca.transform(data_desiredPathology_df[['PCA1','PCA2','PCA3']])\n",
    "\n",
    "# remove the first row from the dataframe\n",
    "data_desiredPathology_df = data_desiredPathology_df.iloc[4:]\n",
    "\n",
    "# create the title of the plot\n",
    "title = f'3D PCA: {histopath} samples for {unique_ids[sampleLabel]}'\n",
    "\n",
    "time = data_desiredPathology_df['Time']\n",
    "# Subtract the first time point from the rest of the time points\n",
    "time = time - time.iloc[0]\n",
    "# print the start and end time in minutes and seconds\n",
    "timeStart = time.iloc[0] \n",
    "# Reformat time to minutes and seconds\n",
    "\n",
    "print('Start time: ',time.iloc[0],' minutes')\n",
    "print('End time: ',time.iloc[-1],' minutes')\n",
    "\n",
    "# plot the cancer samples with the colour corresponding to the time column\n",
    "SECONDS_PER_MINUTE = 60\n",
    "fig = px.scatter_3d(data_desiredPathology_df, x='PCA1', y='PCA2', z='PCA3',title=title, color=time/SECONDS_PER_MINUTE, opacity=0.8, color_discrete_sequence=['yellow','blue','orange','pink','red'])\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(width=np.floor(figSize[0]*1.5), height=figSize[1],title_x=0.5, title_y=0.8,legend=dict(x=0.8, y=0.8),coloraxis_colorbar=dict(x=0.8, title='Time (minutes)'))\n",
    "\n",
    "fig.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "bigPlot = True\n",
    "if bigPlot:\n",
    "    figSize = (700,700)\n",
    "else:\n",
    "    figSize = (400,400)\n",
    "# Analyzing the shift of the cancer over time for each sample\n",
    "histopathologyLabel = 0 # 0 for normal, 1 for cancer\n",
    "for histopathologyLabel in range(2):\n",
    "    #if histopathologylabel is 0, histopathologyLabelText is 'normal', else it is normal \n",
    "    if histopathologyLabel ==0:\n",
    "        histopath = 'Normal'\n",
    "    else:\n",
    "        histopath = 'Cancer'\n",
    "\n",
    "    # Get the desired histopathology from dataframe\n",
    "    data_desiredPathology_df_saved = data_pca_df[data_pca_df['Label (numeric)'] == histopathologyLabel].copy()\n",
    "    # Get uniqiue ids from the SampleID column\n",
    "    unique_ids = data_desiredPathology_df_saved['SampleID'].unique()\n",
    "    print(f'OVERVIEW INFORMATION for {histopath}:')\n",
    "    print(f'Samples which contain {histopath}:',unique_ids)\n",
    "    # here is the current object Index(['PatientID', 'SampleID', 'Label (numeric)', 'Label', 'Data', 'Time', 'Data_preprocessed', 'PCA1', 'PCA2', 'PCA3'], dtype='object')\n",
    "    # Add an additional empty column to the dataframe that contains the absolute time to be calculated from the time column\n",
    "    data_desiredPathology_df_saved['Time_abs'] = np.nan\n",
    "    data_desiredPathology_df_saved.head()\n",
    "\n",
    "    # print size of the dataframe\n",
    "    print('REMOVE TEST READINGS FROM EACH SAMPLE SET')\n",
    "    print('Size of dataframe before removing samples:',data_desiredPathology_df_saved.shape)\n",
    "    # for each sample in data_desiredPathology_df_saved, remove the first x samples from the dataframe\n",
    "    if histopathologyLabel == 1:\n",
    "        samples_to_remove = [1,1,1,4,3,4]\n",
    "    else:\n",
    "        samples_to_remove = [0,1,0,0]\n",
    "    for sampleLabel in range(len(samples_to_remove)):\n",
    "        n = samples_to_remove[sampleLabel]\n",
    "        labelName = unique_ids[sampleLabel]\n",
    "        # get the index of the first n occurences of labelName\n",
    "        indexNames = data_desiredPathology_df_saved[data_desiredPathology_df_saved['SampleID'] == labelName].iloc[:n].index\n",
    "        # Delete these row indexes from dataFrame\n",
    "        data_desiredPathology_df_saved.drop(indexNames , inplace=True)\n",
    "        # print which rows were deleted\n",
    "    print('Size of dataframe after removing samples:',data_desiredPathology_df_saved.shape)\n",
    "\n",
    "    # For each sample, subtract the first time point from the rest of the time points\n",
    "    for sampleLabel in range(len(samples_to_remove)):\n",
    "        # Get desired sample from dataset\n",
    "        data_desiredPathology_df = data_desiredPathology_df_saved[data_desiredPathology_df_saved['SampleID'] == unique_ids[sampleLabel]].copy()\n",
    "        time = data_desiredPathology_df['Time']\n",
    "        start_time = time.iloc[0]\n",
    "        # For each time in the data_desiredPathology_df, subtract the start time\n",
    "        time_abs = time - start_time\n",
    "        # Add the time_abs column to the data_desiredPathology_df\n",
    "        SECONDS_PER_MINUTE = 60\n",
    "        data_desiredPathology_df['Time_abs'] = time_abs/SECONDS_PER_MINUTE\n",
    "        # merge this back into the data_desiredPathology_df_saved\n",
    "        data_desiredPathology_df_saved[data_desiredPathology_df_saved['SampleID'] == unique_ids[sampleLabel]] = data_desiredPathology_df\n",
    "    data_desiredPathology_df_saved\n",
    "\n",
    "    # create the title of the plot\n",
    "    title = f'3D PCA: {histopath} samples over time'\n",
    "    # plot the cancer samples with the colour corresponding to the time column\n",
    "    SECONDS_PER_MINUTE = 60\n",
    "    fig = px.scatter_3d(data_desiredPathology_df_saved, x='PCA1', y='PCA2', z='PCA3',title=title, color='Time_abs', opacity=0.8, color_continuous_scale='viridis')\n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    fig.update_layout(width=np.floor(figSize[0]*1.5), height=figSize[1],title_x=0.5, title_y=0.8,legend=dict(x=0.8, y=0.8),coloraxis_colorbar=dict(x=0.8, title='Time (minutes)'))\n",
    "    fig.show()\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigPlot = True\n",
    "if bigPlot:\n",
    "    figSize = (700,700)\n",
    "else:\n",
    "    figSize = (400,400)\n",
    "# Analyzing the shift of the cancer over time for each sample\n",
    "histopathologyLabel = 1 # 0 for normal, 1 for cancer\n",
    "for histopathologyLabel in range(2):\n",
    "    #if histopathologylabel is 0, histopathologyLabelText is 'normal', else it is normal \n",
    "    if histopathologyLabel ==0:\n",
    "        histopath = 'Normal'\n",
    "    else:\n",
    "        histopath = 'Cancer'\n",
    "\n",
    "    # Get the desired histopathology from dataframe\n",
    "    data_desiredPathology_df_saved = data_pca_df[data_pca_df['Label (numeric)'] == histopathologyLabel].copy()\n",
    "    # Get uniqiue ids from the SampleID column\n",
    "    unique_ids = data_desiredPathology_df_saved['SampleID'].unique()\n",
    "    print(f'OVERVIEW INFORMATION for {histopath}:')\n",
    "    print(f'Samples which contain {histopath}:',unique_ids)\n",
    "    # here is the current object Index(['PatientID', 'SampleID', 'Label (numeric)', 'Label', 'Data', 'Time', 'Data_preprocessed', 'PCA1', 'PCA2', 'PCA3'], dtype='object')\n",
    "    # Add an additional empty column to the dataframe that contains the absolute time to be calculated from the time column\n",
    "    data_desiredPathology_df_saved['Time_abs'] = np.nan\n",
    "    data_desiredPathology_df_saved.head()\n",
    "\n",
    "    # print size of the dataframe\n",
    "    print('REMOVE TEST READINGS FROM EACH SAMPLE SET')\n",
    "    print('Size of dataframe before removing samples:',data_desiredPathology_df_saved.shape)\n",
    "    # for each sample in data_desiredPathology_df_saved, remove the first x samples from the dataframe\n",
    "    if histopathologyLabel == 1:\n",
    "        samples_to_remove = [1,1,1,4,3,4]\n",
    "    else:\n",
    "        samples_to_remove = [0,1,0,0]\n",
    "    for sampleLabel in range(len(samples_to_remove)):\n",
    "        n = samples_to_remove[sampleLabel]\n",
    "        labelName = unique_ids[sampleLabel]\n",
    "        # get the index of the first n occurences of labelName\n",
    "        indexNames = data_desiredPathology_df_saved[data_desiredPathology_df_saved['SampleID'] == labelName].iloc[:n].index\n",
    "        # Delete these row indexes from dataFrame\n",
    "        data_desiredPathology_df_saved.drop(indexNames , inplace=True)\n",
    "        # print which rows were deleted\n",
    "    print('Size of dataframe after removing samples:',data_desiredPathology_df_saved.shape)\n",
    "\n",
    "    # For each sample, subtract the first time point from the rest of the time points\n",
    "    for sampleLabel in range(len(samples_to_remove)):\n",
    "        # Get desired sample from dataset\n",
    "        data_desiredPathology_df = data_desiredPathology_df_saved[data_desiredPathology_df_saved['SampleID'] == unique_ids[sampleLabel]].copy()\n",
    "        time = data_desiredPathology_df['Time']\n",
    "        start_time = time.iloc[0]\n",
    "        # For each time in the data_desiredPathology_df, subtract the start time\n",
    "        time_abs = time - start_time\n",
    "        # Add the time_abs column to the data_desiredPathology_df\n",
    "        SECONDS_PER_MINUTE = 60\n",
    "        data_desiredPathology_df['Time_abs'] = time_abs/SECONDS_PER_MINUTE\n",
    "        # merge this back into the data_desiredPathology_df_saved\n",
    "        data_desiredPathology_df_saved[data_desiredPathology_df_saved['SampleID'] == unique_ids[sampleLabel]] = data_desiredPathology_df\n",
    "\n",
    "\n",
    "    import plotly.graph_objs as go\n",
    "    # Make a dictionary of the shapes for each sample label\n",
    "    symbol_dict = {'Sample1_back': 'circle',\n",
    "                'Sample1_front': 'circle',\n",
    "                'Sample2_back': 'square',\n",
    "                'Sample2_front': 'square',\n",
    "                'Sample3_back': 'diamond',\n",
    "                'Sample3_front': 'diamond'\n",
    "                }\n",
    "\n",
    "    # create a trace for each sample label with different marker shapes\n",
    "    traces = []\n",
    "    for label in np.unique(data_desiredPathology_df_saved['SampleID']):\n",
    "        idx = data_desiredPathology_df_saved['SampleID'] == label\n",
    "        trace = go.Scatter3d(\n",
    "            x=data_desiredPathology_df_saved['PCA1'][idx],\n",
    "            y=data_desiredPathology_df_saved['PCA2'][idx],\n",
    "            z=data_desiredPathology_df_saved['PCA3'][idx],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=5, symbol=symbol_dict[label], opacity=0.8, color=data_desiredPathology_df_saved['Time_abs'][idx],\n",
    "                colorscale='viridis', colorbar=dict(title='Time (minutes)'), coloraxis='coloraxis'\n",
    "            ),\n",
    "            name=label,\n",
    "            hovertemplate='PCA1: %{x:.2f}<br>PCA2: %{y:.2f}<br>PCA3: %{z:.2f}<br>Time: %{marker.color:.2f} minutes'\n",
    "        )\n",
    "        traces.append(trace)\n",
    "\n",
    "    # define the layout of the plot\n",
    "    layout = go.Layout(\n",
    "        title=dict(x=0.5, y=0.8),#,text=f'3D PCA: {histopath} samples over time'),\n",
    "        width=np.floor(figSize[0]*1.5),\n",
    "        height=figSize[1],\n",
    "        scene=dict(xaxis_title='PCA1', yaxis_title='PCA2', zaxis_title='PCA3')#, xaxis=dict(range=[-5, 10]), yaxis=dict(range=[-4, 4]), zaxis=dict(range=[-1, 3])),\n",
    "        ,legend=dict(x=0.05, y=0.8),\n",
    "        coloraxis=dict(colorbar=dict(x=0.8, title='Time (minutes)'), colorscale='viridis'),#  'jet'),\n",
    "\n",
    "    )\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot the cancer samples with the colour corresponding to the time column\n",
    "# # Set marker symbols for each unique sample label\n",
    "# if histopathologyLabel == 1:\n",
    "#     symbol_sequence = ['circle','circle-open','square','square-open', 'diamond','diamond-open']\n",
    "#     symbol_sequence = ['circle','circle','circle','circle','circle','circle']\n",
    "\n",
    "# else:\n",
    "#     symbol_sequence = ['circle','circle-open','diamond','diamond-open']\n",
    "#     symbol_sequence = ['circle', 'circle', 'circle', 'circle']\n",
    "\n",
    "# # create the title of the plot\n",
    "# title = f'3D PCA: {histopath} samples over time'\n",
    "# #plot the cancer samples with the colour corresponding to the time column and different marker shapes for each sample label\n",
    "# fig = px.scatter_3d(data_desiredPathology_df_saved, x='PCA1', y='PCA2', z='PCA3',\n",
    "#                     title=title, color='Time_abs', opacity=0.8, color_continuous_scale='viridis',\n",
    "#                     symbol='SampleID', symbol_sequence=symbol_sequence)\n",
    "# fig.update_traces(marker=dict(size=5))\n",
    "# fig.update_layout(\n",
    "#     # Change figure dimensions\n",
    "#     width=np.floor(figSize[0]*1.5), \n",
    "#     height=figSize[1],\n",
    "#     # Change element positions\n",
    "#     title_x=0.5, \n",
    "#     title_y=0.8,\n",
    "#     legend=dict(x=0.1, y=0.8),\n",
    "#     coloraxis_colorbar=dict(x=0.8, title='Time (minutes)'),\n",
    "# )\n",
    "# fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fe5cc0d",
   "metadata": {},
   "source": [
    "## 3D LDA trials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6161ac39",
   "metadata": {},
   "source": [
    "### 3D LDA - data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f9223",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Description: LDA dimensionality is limited by the number of classes. \n",
    "To create a 3D LDA we need to split each cancer and normal class into two classes.\n",
    "\n",
    "'''\n",
    "# Create a copy of the dataframe\n",
    "proData_3DLDA_df = processedData_df.copy()\n",
    "# For Label (numeric label) = 1, change half the Labels to 'cancer1' and the other half to 'cancer2'\n",
    "proData_3DLDA_df.loc[proData_3DLDA_df['Label (numeric)'] == 1, 'Label'] = np.where(np.random.rand(len(proData_3DLDA_df[proData_3DLDA_df['Label (numeric)'] == 1])) > 0.5, 'cancer1', 'cancer2')\n",
    "# Also renumber the numberic label from 1 to 2 for cancer1 and 3 for cancer2\n",
    "proData_3DLDA_df.loc[proData_3DLDA_df['Label'] == 'cancer1', 'Label (numeric)'] = 2\n",
    "proData_3DLDA_df.loc[proData_3DLDA_df['Label'] == 'cancer2', 'Label (numeric)'] = 3\n",
    "\n",
    "# Do the same for normal but change the numeric label to 0 and 1\n",
    "proData_3DLDA_df.loc[proData_3DLDA_df['Label (numeric)'] == 0, 'Label'] = np.where(np.random.rand(len(proData_3DLDA_df[proData_3DLDA_df['Label (numeric)'] == 0])) > 0.5, 'normal1', 'normal2')\n",
    "proData_3DLDA_df.loc[proData_3DLDA_df['Label'] == 'normal1', 'Label (numeric)'] = 0\n",
    "proData_3DLDA_df.loc[proData_3DLDA_df['Label'] == 'normal2', 'Label (numeric)'] = 1\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "185cd10f",
   "metadata": {},
   "source": [
    "### 3D LDA - All data for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994dc3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# define font sizes\n",
    "fontTitle = 20\n",
    "fontLabel = 16\n",
    "\n",
    "# Proceed with LDA on the data\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Get the data from the dataframe\n",
    "data = np.array(proData_3DLDA_df['Data_preprocessed'].tolist())[:,:,1]\n",
    "# Get the labels from the dataframe\n",
    "labels = np.array(proData_3DLDA_df['Label (numeric)'].tolist())\n",
    "\n",
    "# Create the LDA object\n",
    "lda = LinearDiscriminantAnalysis(n_components=3)\n",
    "# Fit the LDA object to the data and labels\n",
    "lda.fit(data, labels)\n",
    "# Transform the data\n",
    "data_lda = lda.transform(data)\n",
    "print(data_lda.shape)\n",
    "# Create a dataframe with the LDA data\n",
    "data_lda_df = pd.DataFrame(data_lda, columns=['LDA1','LDA2','LDA3'])\n",
    "# Add the LDA data to the dataframe\n",
    "data_3Dlda_df = pd.concat([proData_3DLDA_df, data_lda_df], axis=1)\n",
    "# data_3Dlda_df\n",
    "\n",
    "# 3D LDA plot of cancer vs normal\n",
    "labels = np.unique(data_3Dlda_df['Label'])\n",
    "\n",
    "fig = px.scatter_3d(data_3Dlda_df, x='LDA1', y='LDA2', z='LDA3', color='Label', opacity=0.7, width=figSize[0], height=figSize[1]\n",
    "                    ,color_discrete_sequence=['yellow','blue','orange','pink'])\n",
    "fig.update_layout(title='3D LDA: cancer vs normal', title_x=0.5, title_font_size=fontTitle, legend_title_text='Label', legend_title_font_size=fontLabel, font_size=fontLabel)\n",
    "# lower the title and the legend\n",
    "fig.update_layout(title_y=0.7, legend_y=0.7, legend_x=0.8)\n",
    "fig.show()\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb75be98",
   "metadata": {},
   "source": [
    "### 3D LDA - Train test split experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1006d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create3DLDA(trainData_df, testData_df, figSize,title,aspectratio=dict(x=1, y=1, z=1)):\n",
    "\n",
    "    # instantiate the LDA object\n",
    "    lda = LinearDiscriminantAnalysis(n_components=3)\n",
    "    # Train data\n",
    "    train_X = np.array(trainData_df['Data_preprocessed'].tolist())[:,:,1]\n",
    "    train_y = np.array(trainData_df['Label (numeric)'].tolist())\n",
    "    # fit the LDA object to the data and labels\n",
    "    lda.fit(train_X, train_y)\n",
    "\n",
    "    # Test data\n",
    "    test_X = np.array(testData_df['Data_preprocessed'].tolist())[:,:,1]\n",
    "    # Transform the data\n",
    "    test_X_lda = lda.transform(test_X)\n",
    "    # Create a dataframe with the LDA data\n",
    "    test_X_lda_df = pd.DataFrame(test_X_lda, columns=['LDA1','LDA2','LDA3'])\n",
    "    # copy the backData_df and add the LDA data to the dataframe\n",
    "    test_X_lda_df = pd.concat([testData_df, test_X_lda_df], axis=1)\n",
    "    # 3D LDA plot of cancer vs normal\n",
    "    fig = px.scatter_3d(\n",
    "        test_X_lda_df, \n",
    "        x='LDA1', y='LDA2', z='LDA3', \n",
    "        color=test_X_lda_df['Label'], \n",
    "        opacity=0.7, width=figSize[0], height=figSize[1],\n",
    "        color_discrete_sequence=['orange','orange','blue','blue'],\n",
    "        symbol=test_X_lda_df['Label'], symbol_sequence=['circle','circle-open','square','square-open'],\n",
    "        # symbol=test_X_lda_df['SampleID'], symbol_sequence=['circle','circle-open','square','square-open','diamond','diamond-open']\n",
    "        # symbol='SampleID', symbol_sequence=['circle', 'circle-open','square','square-open']\n",
    "        hover_data=['SampleID','LDA1','LDA2','LDA3','Label'],\n",
    "    )\n",
    "    fig.update_layout(title=dict(text=title, x=0.5, y=0.7), legend = dict(title='Label',y=0.7, x=0.8))\n",
    "    # update the axes so they have the same scale\n",
    "    # fig.update_layout(scene_aspectmode='cube')\n",
    "    fig.update_layout(scene=dict(aspectratio=aspectratio))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "bigPlot = True\n",
    "if bigPlot:\n",
    "    figSize = (700,700)\n",
    "else:\n",
    "    figSize = (400,400)\n",
    "# 3D LDA - Train on back test of front\n",
    "\n",
    "# Get the front and back data separately. The SampleID will contain front or back\n",
    "frontData_df = proData_3DLDA_df[proData_3DLDA_df['SampleID'].str.contains('front')].reset_index(drop=True)\n",
    "backData_df = proData_3DLDA_df[proData_3DLDA_df['SampleID'].str.contains('back')].reset_index(drop=True)\n",
    "\n",
    "'''Experiment 1: random train test splits'''\n",
    "# split the train and test randomly 30% test, 70% train\n",
    "trainData_df, testData_df = train_test_split(proData_3DLDA_df, test_size=0.3, random_state=41)\n",
    "# create3DLDA(trainData_df=trainData_df.reset_index(drop=True), testData_df=testData_df.reset_index(drop=True), figSize=figSize, title='LDA: Random Train70 - Test30')\n",
    "# Remove sample 1 back from the data and run the LDA again\n",
    "trainData_df = trainData_df[trainData_df['SampleID'] != 'Sample1_back'].reset_index(drop=True)\n",
    "testData_df = testData_df[testData_df['SampleID'] != 'Sample1_back'].reset_index(drop=True)\n",
    "create3DLDA(trainData_df=trainData_df.reset_index(drop=True), testData_df=testData_df.reset_index(drop=True), figSize=figSize, title='LDA: Random Train70 - Test30, Sample1_back removed')\n",
    "\n",
    "'''Experiment 2: train on back, test on front'''\n",
    "trainData_df = frontData_df\n",
    "testData_df = backData_df\n",
    "# create3DLDA(trainData_df=frontData_df, testData_df=backData_df, figSize=figSize, title='LDA: Train-Test, Front-Back')\n",
    "# create3DLDA(trainData_df=backData_df, testData_df=frontData_df, figSize=figSize, title='LDA: Train=Test, Back-Front')\n",
    "\n",
    "'''Experiment 3: leave 1 out cross validation. Train on 5 samples, test on 1 sample'''\n",
    "# Get the unique sample IDs\n",
    "unique_ids = proData_3DLDA_df['SampleID'].unique()\n",
    "# systematically create a train and test set\n",
    "for i in range(len(unique_ids)):\n",
    "    # get the test sample\n",
    "    testData_df = proData_3DLDA_df[proData_3DLDA_df['SampleID'] == unique_ids[i]].reset_index(drop=True)\n",
    "    # get the train samples\n",
    "    trainData_df = proData_3DLDA_df[proData_3DLDA_df['SampleID'] != unique_ids[i]].reset_index(drop=True)\n",
    "    # create3DLDA(trainData_df=trainData_df, testData_df=testData_df, figSize=figSize, title=f'LDA: Leave 1 out validation test sample: {unique_ids[i]}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b89e61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13012e37",
   "metadata": {},
   "source": [
    "### PCA LDA classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8f4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PCA LDA functions'''\n",
    "'''\n",
    "Now that I have interrogated PCA and LDA separately through a graphical representation, I will now combine the two methods to see if I can improve the classification accuracy.\n",
    "Steps:\n",
    "1. Get the data from the processedData_df and store it in a new dataframe called data_PCALDA_df\n",
    "2. Separate the data into train and test sets\n",
    "3. Perform PCA on the train data - selecting a subset of principal components\n",
    "4. Fit a LDA classifier to the PCA data\n",
    "5. Evaluate the performance of the PCA-LDA classifier on the test data\n",
    "\n",
    "'''\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "''' The folowing function creates a PCA LDA pipeline and returns the fit pipeline'''\n",
    "def fit_PCA_LDA(trainData_df, n_components=190):\n",
    "    # extract the train data\n",
    "    # Extract Train data\n",
    "    train_X = np.array(trainData_df['Data_preprocessed'].tolist())[:,:,1]\n",
    "    train_y = np.array(trainData_df['Label (numeric)'].tolist())\n",
    "\n",
    "    # n_components must be between 0 and min(n_samples, n_features)=279\n",
    "    n_features = train_X.shape[1]\n",
    "    n_samples = train_X.shape[0]\n",
    "    if n_components > min(n_samples, n_features):\n",
    "        n_components = int(np.ceil(min(n_samples, n_features)*(2/3)))\n",
    "        np.ceil\n",
    "        print(f'n_components must be between 0 and min(n_samples, n_features), set to 2/3 max: {n_components}')\n",
    "\n",
    "    # Step 1: Perform PCA on training data\n",
    "    pca = PCA(n_components=n_components) # Note the recommended number of components is 1/3 of the total number of features - about 1000 here\n",
    "    pca.fit_transform(train_X)\n",
    "    train_X_pca = pca.transform(train_X)\n",
    "\n",
    "    # Step 2: Perform LDA on the transformed data to build a classifier\n",
    "    lda = LDA()\n",
    "    lda.fit(train_X_pca, train_y)\n",
    "    pca_lda_classifier = make_pipeline(pca, lda)\n",
    "\n",
    "    # return the trained classifier\n",
    "    return pca_lda_classifier\n",
    "\n",
    "''' The following function takes in test data and a fit PCA-LDA pipeline and returns the results of the classifier'''\n",
    "def evaluate_PCA_LDA(testData_df, classifier, expName='Specify classifier type', average='binary', BY_SAMPLE=False, CM_FLAG=True):\n",
    "    print(f'{expName} Results:')\n",
    "    # Extract Test data\n",
    "    test_X = np.array(testData_df['Data_preprocessed'].tolist())[:, :, 1]\n",
    "    test_y = np.array(testData_df['Label (numeric)'].tolist())\n",
    "\n",
    "    # Evaluate the performance of the classifier on the test data\n",
    "    y_pred = classifier.predict(test_X)\n",
    "    # Add the predicted labels to the dataframe\n",
    "    testData_df['Predicted Label'] = y_pred # I think this is the source of the warning\n",
    "\n",
    "    # Calculate all of the desired metrics: Accuracy, Precision, Recall, F1\n",
    "    accuracy = accuracy_score(test_y, y_pred)\n",
    "    precision = precision_score(test_y, y_pred, average=average)\n",
    "    recall = recall_score(test_y, y_pred, average=average)\n",
    "    f1 = f1_score(test_y, y_pred, average=average)\n",
    "    # display the calculated results in an aesthetic table\n",
    "    results_df = pd.DataFrame({'Accuracy': [accuracy], 'Precision' : [precision], 'Recall' : [recall], 'F1' : [f1]}).round({'Value': 2})\n",
    "    display(results_df)\n",
    "\n",
    "    if CM_FLAG:\n",
    "        # Display the overall confusion matrix\n",
    "        cm = confusion_matrix(test_y, y_pred)\n",
    "        # show shape of cm\n",
    "        print('Confusion Matrix Shape:',cm.shape)\n",
    "        unique_testy = np.unique(test_y)\n",
    "        unique_predy = np.unique(y_pred)\n",
    "        print('Unique test_y:',unique_testy)\n",
    "        print('Unique y_pred:',unique_predy)\n",
    "        classes = np.unique(np.concatenate((unique_testy,unique_predy)))\n",
    "        # classes = ['normal', 'cancer']\n",
    "        # classes = [1,2, 3]\n",
    "        cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "        display(cm)\n",
    "        displayCM(cm_df, expName=expName)\n",
    "\n",
    "    if BY_SAMPLE:\n",
    "        # Display the confusion matrix broken down by sampleID\n",
    "        cm_dfs, sampleIDs = cmSampleBreakdown(testData_df)\n",
    "        for i in range(len(cm_dfs)):\n",
    "            displayCM(cm_dfs[i], expName=f'{expName}: {sampleIDs[i]}')\n",
    "\n",
    "\n",
    "    return results_df\n",
    "# define a function to create the confusion matrix breakdown by sampleID\n",
    "def cmSampleBreakdown(testData_df):\n",
    "    '''\n",
    "    Display the confusion matrix broken down by sampleID.\n",
    "    This offers much more infomration about where the incorrect classifications came from.\n",
    "    The predictions are stored in the 'Predicted Label' column\n",
    "    The actual labels are stored in the 'Label (numeric)' column\n",
    "    '''\n",
    "    unique_ids = testData_df['SampleID'].unique()\n",
    "    cm_dfs = []\n",
    "    for i in range(len(unique_ids)):\n",
    "        # get the actual labels for the sample\n",
    "        actual_labels = testData_df[testData_df['SampleID'] == unique_ids[i]]['Label (numeric)']\n",
    "        # get the predicted labels for the sample\n",
    "        predicted_labels = testData_df[testData_df['SampleID'] == unique_ids[i]]['Predicted Label']\n",
    "        # calculate the confusion matrix for the sample\n",
    "        cm = confusion_matrix(actual_labels, predicted_labels)\n",
    "        print(cm)\n",
    "        # classes = ['normal', 'cancer']\n",
    "        # get the classes from the actual labels\n",
    "        classes = np.unique(actual_labels)      \n",
    "        cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "        cm_dfs.append(cm_df)\n",
    "    return cm_dfs,unique_ids\n",
    "\n",
    "def displayCM(cm_df, expName='PCA-LDA'):\n",
    "    plt.figure()\n",
    "    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.title(f'{expName} Confusion Matrix')\n",
    "    # update the size of the text\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d7a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust plot sizes\n",
    "bigPlot = False\n",
    "if bigPlot:\n",
    "    figSize = (700,700)\n",
    "else:\n",
    "    figSize = (400,400)\n",
    "\n",
    "#Store the processed data for use in the PCA LDA workflow\n",
    "data_PCALDA_df = processedData_df.copy()\n",
    "# Add the sample number to the dataframe\n",
    "data_PCALDA_df['Sample Number'] = data_PCALDA_df['SampleID'].str.extract('(\\d+)').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b7a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Experiment 1.1: Random train test split'''\n",
    "# Split the data into train and test sets\n",
    "trainData_df, testData_df = train_test_split(data_PCALDA_df, test_size=0.3, random_state=41)\n",
    "# fit the model to the train data\n",
    "classifier = fit_PCA_LDA(trainData_df=trainData_df, n_components=190)\n",
    "# Evaluate the model on the test data\n",
    "# results_df = evaluate_PCA_LDA(testData_df=testData_df, classifier=classifier, expName='PCA-LDA: Class separation Random 70/30')\n",
    "\n",
    "''' Experiment 1.2: Random train test split but use sample number as class'''\n",
    "# copy to a new dataframe, this is just for this experiment\n",
    "data_SamplesAsClasses_df = data_PCALDA_df.copy()\n",
    "# Add a column to the dataframe that contains the sample number\n",
    "data_SamplesAsClasses_df['Label (numeric)'] = data_SamplesAsClasses_df['Sample Number'].astype(int) #- 1\n",
    "# Pupose of the '(\\d+)' argument is to extract the number from the string, the -1 is to make the sample number start at 0 instead of 1\n",
    "# Split the data into train and test sets\n",
    "trainData_df, testData_df = train_test_split(data_SamplesAsClasses_df, test_size=0.3, random_state=41)\n",
    "# fit the model to the train data\n",
    "classifier = fit_PCA_LDA(trainData_df=trainData_df, n_components=190)\n",
    "# Evaluate the model on the test data\n",
    "# results_df = evaluate_PCA_LDA(testData_df=testData_df, pca_lda_classifier=pca_lda_classifier, expName='PCA-LDA: Sample separation Random 70/30', average='macro')\n",
    "\n",
    "'''Experiment 1.3: Use cancer as train and normal as test. Try to classify by sample number'''\n",
    "# Get the dataframe\n",
    "data_SamplesAsClasses_df = data_PCALDA_df.copy()\n",
    "# split the data into cancer and normal\n",
    "data_cancer_df = data_SamplesAsClasses_df[data_SamplesAsClasses_df['Label (numeric)'] == 1]\n",
    "data_normal_df = data_SamplesAsClasses_df[data_SamplesAsClasses_df['Label (numeric)'] == 0]\n",
    "# set the sample number as the class\n",
    "data_cancer_df['Label (numeric)'] = data_cancer_df['Sample Number']\n",
    "data_normal_df['Label (numeric)'] = data_normal_df['Sample Number']\n",
    "classifier = fit_PCA_LDA(trainData_df=data_cancer_df, n_components=190)\n",
    "# Evaluate the model on the test data\n",
    "# results_df = evaluate_PCA_LDA(testData_df=data_normal_df, pca_lda_classifier=pca_lda_classifier, expName='PCA-LDA: Sample separation - Cancer/Normal', average='macro')\n",
    "\n",
    "classifier = fit_PCA_LDA(trainData_df=data_normal_df, n_components=190)\n",
    "# Evaluate the model on the test data\n",
    "# results_df = evaluate_PCA_LDA(testData_df=data_cancer_df, pca_lda_classifier=pca_lda_classifier, expName='PCA-LDA: Sample separation - Normal/Cancer ', average='macro')\n",
    "\n",
    "''' Experiment: Sweeping n_components for PCA: run the model for different numbers of components and plot the results'''\n",
    "# # Create a list of the number of components to test, from 1 to 279\n",
    "n_components_list = np.arange(1, 280, 1)\n",
    "'''\n",
    "# load the results from a file instead of rerunning the code\n",
    "# results_df = pd.read_csv('PCA_ncomponents_Sweep_RandomTrainTest_.csv')\n",
    "# display(results_df)\n",
    "# # Create a list to store the results\n",
    "# results_list = []\n",
    "# # print the length of the list\n",
    "# print(f'Number of components to test: {len(n_components_list)}')\n",
    "# # For each number of components, fit the model and evaluate the model\n",
    "# for n_components in n_components_list:\n",
    "#     # fit the model to the train data\n",
    "#     pca_lda_classifier = fit_PCA_LDA(trainData_df=trainData_df, n_components=n_components)\n",
    "#     # Evaluate the model on the test data\n",
    "#     results_df = evaluate_PCA_LDA(testData_df=testData_df, pca_lda_classifier=pca_lda_classifier, expName=f'PCA-LDA: n_components={n_components}')\n",
    "#     # Append the results to the results_list\n",
    "#     results_list.append(results_df)\n",
    "\n",
    "# # Create a dataframe of the results\n",
    "# results_df = pd.concat(results_list, axis=0).reset_index(drop=True)\n",
    "\n",
    "# create a graph that shows all of the metrics on a single graph\n",
    "if False:\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=n_components_list, y=results_df['Accuracy'], name='Accuracy'))\n",
    "    fig.add_trace(go.Scatter(x=n_components_list, y=results_df['Precision'], name='Precision'))\n",
    "    fig.add_trace(go.Scatter(x=n_components_list, y=results_df['Recall'], name='Recall'))\n",
    "    fig.add_trace(go.Scatter(x=n_components_list, y=results_df['F1'], name='F1'))\n",
    "    fig.update_layout(title='PCA-LDA: Metrics vs n_components', title_x=0.5, xaxis_title='n_components', yaxis_title='Metric')\n",
    "    fig.update_layout(width=figSize[0], height=figSize[1], legend=dict(x=0.8, y=0.8))\n",
    "    fig.show()\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a54bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Experiment 2: Train on back, test on front'''\n",
    "# Get the front and back data separately. The SampleID will contain front or back\n",
    "frontData_df = data_PCALDA_df[data_PCALDA_df['SampleID'].str.contains('front')].reset_index(drop=True)\n",
    "backData_df = data_PCALDA_df[data_PCALDA_df['SampleID'].str.contains('back')].reset_index(drop=True)\n",
    "# fit the model to the train data and evalute on the test data\n",
    "classifier = fit_PCA_LDA(trainData_df=backData_df, n_components=190)\n",
    "# results_df = evaluate_PCA_LDA(testData_df=frontData_df, pca_lda_classifier=pca_lda_classifier, BY_SAMPLE=True,CM_FLAG=False,expName='PCA-LDA: Train-Test, Back-Front')\n",
    "\n",
    "# Repeat for train on front, test on back\n",
    "classifier = fit_PCA_LDA(trainData_df=frontData_df, n_components=190)\n",
    "# results_df = evaluate_PCA_LDA(testData_df=backData_df, pca_lda_classifier=pca_lda_classifier, BY_SAMPLE=True,CM_FLAG=True,expName='PCA-LDA: Train-Test, Front-Back')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09acc9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Experiment 3.1: k-fold cross validation based on sampleID'''\n",
    "# Get the unique sample IDs\n",
    "unique_ids = data_PCALDA_df['SampleID'].unique()\n",
    "# systematically create a train and test set\n",
    "for i in range(len(unique_ids)):\n",
    "    # get the test sample \n",
    "    testData_df = data_PCALDA_df[data_PCALDA_df['SampleID'] == unique_ids[i]].reset_index(drop=True)\n",
    "    # get the train samples\n",
    "    trainData_df = data_PCALDA_df[data_PCALDA_df['SampleID'] != unique_ids[i]].reset_index(drop=True)\n",
    "    # fit the model to the train data\n",
    "    classifier = fit_PCA_LDA(trainData_df=trainData_df, n_components=190)\n",
    "    # Evaluate the model on the test data\n",
    "    results_df = evaluate_PCA_LDA(testData_df=testData_df, classifier=classifier, expName=f'PCA-LDA: Leave 1 out validation test sample: {unique_ids[i]}')\n",
    "    \n",
    "    # results_df = evaluate_PCA_LDA(testData_df=testData_df, pca_lda_classifier=pca_lda_classifier, expName=f'PCA-LDA: Leave 1 out validation test sample: {unique_ids[i]}')\n",
    "\n",
    "'''Experiment 3.2: k-fold cross validation based on sample number 1,2,3'''\n",
    "# Get the unique sample IDs\n",
    "unique_ids = data_PCALDA_df['Sample Number'].unique()\n",
    "# systematically create a train and test set\n",
    "for i in range(len(unique_ids)):\n",
    "    # get the test sample\n",
    "    testData_df = data_PCALDA_df[data_PCALDA_df['Sample Number'] == unique_ids[i]].reset_index(drop=True)\n",
    "    # get the train samples\n",
    "    trainData_df = data_PCALDA_df[data_PCALDA_df['Sample Number'] != unique_ids[i]].reset_index(drop=True)\n",
    "    # fit the model to the train data\n",
    "    classifier = fit_PCA_LDA(trainData_df=trainData_df, n_components=190)\n",
    "    # Evaluate the model on the test data\n",
    "    # results_df = evaluate_PCA_LDA(testData_df=testData_df, classifier=classifier, expName=f'PCA-LDA: Leave 1 out validation test sample: {unique_ids[i]}')\n",
    "                #  evaluate_PCA_LDA(testData_df, classifier, expName='Specify classifier type', average='binary', BY_SAMPLE=False, CM_FLAG=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b86bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Experiment 4: Evaluating the ability of model to predict which sample a reading came from. Train on cancer and test on normal'''\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af6a0bc0",
   "metadata": {},
   "source": [
    "### Feature reduction experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8222185e",
   "metadata": {},
   "source": [
    "#### PCA - LDA 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d75f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' def evaluate_classifier \n",
    "This function is used to evalute a trained classifier using various metrics\n",
    "Inputs: testData_df: a dataframe containing the test data\n",
    "        classifier: a trained classifier\n",
    "        expName: a string containing the name of the experiment\n",
    "        average: a string containing the type of averaging to use for the precision and recall metrics\n",
    "        BY_SAMPLE: a boolean indicating whether to calculate the metrics by sample or not\n",
    "        CM_FLAG: a boolean indicating whether to plot the confusion matrix or not\n",
    "Outputs: results_df: a dataframe containing the results of the experiment\n",
    "'''\n",
    "\n",
    "# Import balanced accuracy score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def evaluate_classifier(testData_df, classifier, data_column = 'Data_preprocessed',expName='Specify classifier type', average='binary', BY_SAMPLE=False, CM_FLAG=True):\n",
    "    print(f'{expName} Results:')\n",
    "    print('Evaluating on data column:',data_column)\n",
    "    # Extract Test data\n",
    "    if data_column == 'Data_preprocessed':\n",
    "        test_X = np.array(testData_df['Data_preprocessed'].tolist())[:, :, 1]\n",
    "    else:\n",
    "        test_X = np.array(testData_df[data_column].tolist())\n",
    "    test_y = np.array(testData_df['Label (numeric)'].tolist())\n",
    "    print('Shape of the data:',test_X.shape)\n",
    "\n",
    "    # Evaluate the performance of the classifier on the test data\n",
    "    y_pred = classifier.predict(test_X)\n",
    "\n",
    "    # Add the predicted labels to the dataframe\n",
    "    testData_df['Predicted Label'] = y_pred # I think this is the source of the warning\n",
    "\n",
    "    # Calculate all of the desired metrics: Accuracy, Precision, Recall, F1\n",
    "    accuracy = accuracy_score(test_y, y_pred)\n",
    "    precision = precision_score(test_y, y_pred, average=average)\n",
    "    recall = recall_score(test_y, y_pred, average=average)\n",
    "    f1 = f1_score(test_y, y_pred, average=average)\n",
    "    # Calculate the weighted metrics. This is useful when the classes are imbalanced\n",
    "    balanced_accuracy = balanced_accuracy_score(test_y, y_pred)\n",
    "\n",
    "\n",
    "    # display the calculated results in an aesthetic table\n",
    "    results_df = pd.DataFrame({'Balanced accuracy': [balanced_accuracy], 'Precision' : [precision], 'Recall' : [recall], 'F1' : [f1]}).round({'Value': 2})\n",
    "    \n",
    "\n",
    "    if CM_FLAG:\n",
    "        display(results_df)\n",
    "        # Display the overall confusion matrix\n",
    "        cm = confusion_matrix(test_y, y_pred)\n",
    "        # show shape of cm\n",
    "        # print('Confusion Matrix Shape:',cm.shape)\n",
    "        unique_testy = np.unique(test_y)\n",
    "        unique_predy = np.unique(y_pred)\n",
    "        # print('Unique test_y:',unique_testy)\n",
    "        # print('Unique y_pred:',unique_predy)\n",
    "        classes = np.unique(np.concatenate((unique_testy,unique_predy)))\n",
    "        # classes = ['normal', 'cancer']\n",
    "        # classes = [1,2, 3]\n",
    "        cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "        # display(cm)\n",
    "        displayCM(cm_df, expName=expName)\n",
    "\n",
    "    if BY_SAMPLE:\n",
    "        # Display the confusion matrix broken down by sampleID\n",
    "        cm_dfs, sampleIDs = cmSampleBreakdown(testData_df)\n",
    "        for i in range(len(cm_dfs)):\n",
    "            displayCM(cm_dfs[i], expName=f'{expName}: {sampleIDs[i]}')\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14737212",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The process for this experiment is as follows:\n",
    "1. Train test Split\n",
    "    - Split the processedData_df into train_df and test_df\n",
    "2. Feature reduction\n",
    "    - Perform feature reduction using chosen method on train_df\n",
    "    - Input: train_df and test_df\n",
    "    - Output: reduced train_df and test_df (add a column for the reduced data)\n",
    "4. Classification \n",
    "    - Input: train_X, train_y\n",
    "    - Output: trained classifier\n",
    "5. Evaluate the classifier\n",
    "    - Input: trained classifier, test_df\n",
    "'''\n",
    "data_df = processedData_df.copy()\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.3, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Experiment 1: Testing LDA without any feature reduction as baseline '''\n",
    "def fit_LDA(train_df, data_column):\n",
    "    # print('Name of data column for LDA: ',data_column)\n",
    "    # Extract train data\n",
    "    if data_column == 'Data_preprocessed':\n",
    "        train_X = np.array(train_df[data_column].tolist())[:, :, 1]\n",
    "    else:\n",
    "        train_X = np.array(train_df[data_column].tolist())\n",
    "    train_y = np.array(train_df['Label (numeric)'].tolist())\n",
    "    # print('Shape of the data:',train_X.shape)\n",
    "    # fit the model\n",
    "    # lda_classifier = LDA(solver='lsqr', shrinkage='auto')\n",
    "    # lda_classifier = LDA(solver = 'svd')\n",
    "    lda_classifier = LDA()\n",
    "    lda_classifier.fit(train_X, train_y)\n",
    "    return lda_classifier\n",
    "\n",
    "data_df = processedData_df.copy()\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.3, random_state=41)\n",
    "# fit the model\n",
    "lda_classifier = fit_LDA(train_df, data_column = 'Data_preprocessed')\n",
    "# evaluate the model\n",
    "results_df = evaluate_classifier(testData_df=test_df, classifier=lda_classifier, expName='LDA: Baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62674f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Experiment 2: Testing PCA with 100 components '''\n",
    "\n",
    "data_df = processedData_df.copy()\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.3, random_state=41)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "### Feature reduction\n",
    "train_X = np.array(train_df['Data_preprocessed'].tolist())[:, :, 1]\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit_transform(train_X)\n",
    "\n",
    "# Add the reduced data to the train_df\n",
    "train_X_reduced = pca.transform(train_X)\n",
    "train_df['Data_reduced'] = [train_X_reduced[i] for i in range(train_X_reduced.shape[0])]\n",
    "# Add the reduced data to the test_df\n",
    "test_X = np.array(test_df['Data_preprocessed'].tolist())[:, :, 1]\n",
    "test_X_reduced = pca.transform(test_X)\n",
    "test_df['Data_reduced'] = [test_X_reduced[i] for i in range(test_X_reduced.shape[0])]\n",
    "\n",
    "### Fitting Classifier\n",
    "lda = fit_LDA(train_df, data_column='Data_reduced')\n",
    "\n",
    "### Evaluation\n",
    "results_df = evaluate_classifier(testData_df=test_df, classifier=lda, data_column = 'Data_reduced', expName='PCA-LDA: 100 components')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Experiment 3: Resampling to 100 wavelengths '''\n",
    "from scipy import signal\n",
    "def downsample_data(data, wavelengths, num_points):\n",
    "    # Compute the number of data points for the desired resolution\n",
    "    resampled_wavelengths = np.linspace(wavelengths[0], wavelengths[-1], num=num_points)\n",
    "    # Resample the data to the desired resolution using linear interpolation\n",
    "    resampled_data = signal.resample(data, num_points)\n",
    "\n",
    "    return resampled_wavelengths, resampled_data\n",
    "\n",
    "# Example usage\n",
    "spectrum = np.array(processedData_df['Data_preprocessed'].tolist())[0, :, 1]\n",
    "wavelengths = np.array(processedData_df['Data_preprocessed'].tolist())[0, :, 0]\n",
    "\n",
    "data_df = processedData_df.copy()\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.3, random_state=41)\n",
    "# reset the indices\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "### Feature reduction\n",
    "train_X = np.array(train_df['Data_preprocessed'].tolist())[:, :, 1]\n",
    "train_X_wavelengths = np.array(train_df['Data_preprocessed'].tolist())[:, :, 0]\n",
    "print('Shape of the data:',train_X.shape)\n",
    "# Downsample the data to 100 points\n",
    "# loop through the data and downsample each spectrum\n",
    "num_points = 100\n",
    "# make an empty array to store the downsampled data\n",
    "train_X_reduced = np.zeros((train_X.shape[0], num_points))\n",
    "for i in range(train_X.shape[0]):\n",
    "    train_X_reduced[i, :] = downsample_data(train_X[i, :], train_X_wavelengths[i, :], num_points=num_points)[1]\n",
    "print('Shape of the data after downsampling:',train_X.shape)\n",
    "# Add the reduced data to the train_df\n",
    "train_df['Data_reduced'] = [train_X_reduced[i] for i in range(train_X_reduced.shape[0])]\n",
    "\n",
    "### Fitting Classifier\n",
    "lda = fit_LDA(train_df, data_column='Data_reduced')\n",
    "\n",
    "\n",
    "### Evaluation\n",
    "# Downsample the test set\n",
    "test_X = np.array(test_df['Data_preprocessed'].tolist())[:, :, 1]\n",
    "test_X_wavelengths = np.array(test_df['Data_preprocessed'].tolist())[:, :, 0]\n",
    "num_points = 100\n",
    "test_X_reduced = np.zeros((test_X.shape[0], num_points))\n",
    "for i in range(test_X.shape[0]):\n",
    "    test_X_reduced[i, :] = downsample_data(test_X[i, :], test_X_wavelengths[i, :], num_points=num_points)[1]\n",
    "# add the reduced data to the test_df\n",
    "test_df['Data_reduced'] = [test_X_reduced[i] for i in range(test_X_reduced.shape[0])]\n",
    "results_df = evaluate_classifier(testData_df=test_df, classifier=lda, data_column='Data_reduced',expName='Binning-LDA: 100 bins')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Experiment 4: Feature selection techniques using scikitlearn '''\n",
    "'''\n",
    "The following code uses the scikitlearn feature selection techniques to select the 100 most impactful features for classification.\n",
    "\n",
    "1. Start with my full dataset of 279 samples and about 2500 features\n",
    "2. Fit the lda classifier to the data\n",
    "3. use lda.feature_importances_ to get the feature importances\n",
    "4. sort the feature importances and select the top 100 features\n",
    "5. Determine which wavelengths correspond to the top 100 features\n",
    "6. Use the top 100 features to train the classifier\n",
    "7. Reduce the test set to the top 100 features\n",
    "8. Evaluate the classifier on the test set\n",
    "'''\n",
    "# Step 1: Start with the full dataset\n",
    "data_df = processedData_df.copy()\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.3, random_state=41)\n",
    "\n",
    "# Step 2: Fit the LDA classifier to the data\n",
    "lda_classifier = fit_LDA(train_df, data_column='Data_preprocessed')\n",
    "\n",
    "# Step 3: Extract the most discriminative features\n",
    "feature_importances = lda_classifier.coef_[0]\n",
    "# Step 4: Sort the feature importances and select the top 100 features\n",
    "num_features = 100\n",
    "# np.argsort(feature_importances) sorts the array feature_importances in ascending order and returns the indices that would sort the array.\n",
    "top_indices = np.argsort(feature_importances)[::-1][:num_features]\n",
    "\n",
    "# show an example of the features\n",
    "selected_wavelengths = np.array(processedData_df['Data_preprocessed'].tolist())[0, :, 0][top_indices]\n",
    "selected_features = np.array(processedData_df['Data_preprocessed'].tolist())[0, :, 1][top_indices]\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(x=selected_wavelengths, y=selected_features, mode='markers'))\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title='Wavelength (nm)'),\n",
    "    yaxis=dict(title='Intensity (a.u.)'),\n",
    "    title=f'Top {num_features} Selected Wavelengths',\n",
    "    width=figSize[0],\n",
    "    height=figSize[1]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Step 5: Reduce the train and test data to the selected features\n",
    "train_X_top = np.array(train_df['Data_preprocessed'].tolist())[:, :, 1][:, top_indices]\n",
    "test_X_top = np.array(test_df['Data_preprocessed'].tolist())[:, :, 1][:, top_indices]\n",
    "\n",
    "# Step 5.1 Add the reduced data to the train_df\n",
    "train_df['Data_reduced'] = [train_X_top[i] for i in range(train_X_top.shape[0])]\n",
    "# Step 5.2 Add the reduced data to the test_df\n",
    "test_df['Data_reduced'] = [test_X_top[i] for i in range(test_X_top.shape[0])]\n",
    "\n",
    "# Step 6: Fit the classifier to the reduced data\n",
    "lda_classifier = fit_LDA(train_df,data_column='Data_reduced')\n",
    "\n",
    "# Step 7: Evaluate the classifier on the test set\n",
    "results_df = evaluate_classifier(testData_df=test_df, classifier=lda_classifier, data_column='Data_reduced', expName='LDA: Feature Selection')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "decf59e8",
   "metadata": {},
   "source": [
    "# Ablation study - thesis results generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d22db58",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a9eae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Import all of the requried libraries\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# import statistics\n",
    "from statistics import mode,mean\n",
    "from scipy import interpolate\n",
    "import os\n",
    "import os.path\n",
    "import json\n",
    "import time\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# These are all of the libraries that I manually created\n",
    "\n",
    "import IOfunctions as IO\n",
    "import GUIfunctions as GUI\n",
    "import Processfunctions as process\n",
    "\n",
    "\n",
    "# Through 3D slicer\n",
    "# start_index = 0 # starts at 195nm\n",
    "# start_index = 742 # starts at 350nm\n",
    "start_index = 790 # starts at 360nm\n",
    "# start_index = 1070 # starts at 420nm\n",
    "\n",
    "# print pandas version\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e702411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSpectrum(path, col_name=None,start_index=774,end_index=-1,sep=';'):\n",
    "#     df = pd.read_csv(path + name,sep=';',engine='python')\n",
    "    df = pd.read_csv(path,sep=sep,engine='python')\n",
    "#     print(df)\n",
    "    if not(col_name == None):\n",
    "        df[col_name] = df.index\n",
    "    data = df[start_index:end_index]\n",
    "#     print(data)\n",
    "    data_arr = data.to_numpy()\n",
    "    data_arr = np.array(data_arr,dtype='float')\n",
    "    return data_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9424bcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset_df shape:  (399, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>SampleID</th>\n",
       "      <th>Label (numeric)</th>\n",
       "      <th>Label</th>\n",
       "      <th>Data</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PatientA</td>\n",
       "      <td>Sample1_back</td>\n",
       "      <td>1</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>[[360.14014912872346, 0.0055262738350994245], ...</td>\n",
       "      <td>1.677865e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PatientA</td>\n",
       "      <td>Sample1_back</td>\n",
       "      <td>1</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>[[360.14014912872346, 0.0021762760675127974], ...</td>\n",
       "      <td>1.677866e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PatientA</td>\n",
       "      <td>Sample1_back</td>\n",
       "      <td>1</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>[[360.14014912872346, 0.004287867652463011], [...</td>\n",
       "      <td>1.677866e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PatientA</td>\n",
       "      <td>Sample1_back</td>\n",
       "      <td>1</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>[[360.14014912872346, 0.003312623581628423], [...</td>\n",
       "      <td>1.677866e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PatientA</td>\n",
       "      <td>Sample1_back</td>\n",
       "      <td>1</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>[[360.14014912872346, 0.008101678916785034], [...</td>\n",
       "      <td>1.677866e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PatientID      SampleID  Label (numeric)   Label   \n",
       "0  PatientA  Sample1_back                1  Cancer  \\\n",
       "1  PatientA  Sample1_back                1  Cancer   \n",
       "2  PatientA  Sample1_back                1  Cancer   \n",
       "3  PatientA  Sample1_back                1  Cancer   \n",
       "4  PatientA  Sample1_back                1  Cancer   \n",
       "\n",
       "                                                Data          Time  \n",
       "0  [[360.14014912872346, 0.0055262738350994245], ...  1.677865e+09  \n",
       "1  [[360.14014912872346, 0.0021762760675127974], ...  1.677866e+09  \n",
       "2  [[360.14014912872346, 0.004287867652463011], [...  1.677866e+09  \n",
       "3  [[360.14014912872346, 0.003312623581628423], [...  1.677866e+09  \n",
       "4  [[360.14014912872346, 0.008101678916785034], [...  1.677866e+09  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SampleID</th>\n",
       "      <th>Label (numeric)</th>\n",
       "      <th>Baseline Ambient Light</th>\n",
       "      <th>Baseline Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sample1_back</td>\n",
       "      <td>1</td>\n",
       "      <td>[[360.14014912872346, -0.0001484845964835164],...</td>\n",
       "      <td>0.980610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sample1_back</td>\n",
       "      <td>0</td>\n",
       "      <td>[[360.14014912872346, 0.00021889438755883715],...</td>\n",
       "      <td>1.040830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sample1_front</td>\n",
       "      <td>1</td>\n",
       "      <td>[[360.14014912872346, 0.00035107249884581434],...</td>\n",
       "      <td>1.085760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sample1_front</td>\n",
       "      <td>0</td>\n",
       "      <td>[[360.14014912872346, -3.652628082945304e-05],...</td>\n",
       "      <td>0.978271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sample2_back</td>\n",
       "      <td>1</td>\n",
       "      <td>[[360.1401491287235, 0.00020839429144461395], ...</td>\n",
       "      <td>1.071722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SampleID  Label (numeric)   \n",
       "0   Sample1_back                1  \\\n",
       "1   Sample1_back                0   \n",
       "2  Sample1_front                1   \n",
       "3  Sample1_front                0   \n",
       "4   Sample2_back                1   \n",
       "\n",
       "                              Baseline Ambient Light  Baseline Ratio  \n",
       "0  [[360.14014912872346, -0.0001484845964835164],...        0.980610  \n",
       "1  [[360.14014912872346, 0.00021889438755883715],...        1.040830  \n",
       "2  [[360.14014912872346, 0.00035107249884581434],...        1.085760  \n",
       "3  [[360.14014912872346, -3.652628082945304e-05],...        0.978271  \n",
       "4  [[360.1401491287235, 0.00020839429144461395], ...        1.071722  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\GithubProjects\\Spectroscopy_TrackedTissueSensing\\data\\March2022_raw_data\\SLS201L_Spectrum_reformatted.csv\n",
      "(665, 2)\n",
      "(2858, 2)\n"
     ]
    }
   ],
   "source": [
    "''' Data loader '''\n",
    "# Load in the dataset from file\n",
    "dataset_name = 'KidneyData_march3'\n",
    "trialPath = \"C:/Users/David/OneDrive - Queen's University/1 Graduate Studies/1 Thesis Research/KidneyData_march3/March3_KidneyCollectionWithDrRen/Mar03\"\n",
    "# Load in the dataset from file \n",
    "file_name = os.path.join(trialPath, dataset_name + '_Formatted_Dataset.csv')\n",
    "# Load in the dataset\n",
    "Dataset_df = pd.read_csv(file_name)\n",
    "# For each Data, convert the string back to an array\n",
    "Dataset_df['Data'] = Dataset_df['Data'].apply(lambda x: np.array(json.loads(x)))\n",
    "# print the details of the Dataset_df\n",
    "print('Dataset_df shape: ', Dataset_df.shape)\n",
    "display(Dataset_df.head())\n",
    "\n",
    "\n",
    "# Load in the ambient light baselines dataframe\n",
    "file_name = os.path.join(trialPath, 'ambient_baseline_and_ratios.csv')\n",
    "baseline_df = pd.read_csv(file_name)\n",
    "# For each Data, convert the string back to an array\n",
    "baseline_df['Baseline Ambient Light'] = baseline_df['Baseline Ambient Light'].apply(lambda x: np.array(json.loads(x)))\n",
    "baseline_df.shape\n",
    "baseline_df.head()\n",
    "ambient_baseline_df = baseline_df.copy()\n",
    "# display the ambient_baseline_df\n",
    "display(ambient_baseline_df.head())\n",
    "\n",
    "\n",
    "# Load in the characteristic light source output\n",
    "dataPath = \"C:\\GithubProjects\\Spectroscopy_TrackedTissueSensing\\data\\March2022_raw_data\\SLS201L_Spectrum_reformatted.csv\"\n",
    "print(dataPath)\n",
    "lightsource_output_curve = loadSpectrum(dataPath, 'Wavelength', start_index=10, end_index=675, sep=',')\n",
    "print(lightsource_output_curve[:,0:2].shape)\n",
    "# Interpolate such that the downloaded spectrum has the same values of the data\n",
    "x = lightsource_output_curve[:,0]\n",
    "y = lightsource_output_curve[:,1]\n",
    "f = interpolate.interp1d(x,y)\n",
    "xnew = Dataset_df['Data'][1][:,0]\n",
    "ynew = f(xnew)\n",
    "lightsource_output_curve = np.transpose(np.array([xnew,ynew]))\n",
    "print(lightsource_output_curve.shape)\n",
    "\n",
    "# # Plot an example of the data\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(Dataset_df['Data'][1][:,0], Dataset_df['Data'][1][:,1])\n",
    "# plt.title('Example of the data')\n",
    "# plt.xlabel('Time (s)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a842c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(Dataset_df,ambient_baseline_df, FLAG_AMBIENT_LIGHT,FLAG_CROP, FLAG_NORMALIZE, FLAG_TFUNC):\n",
    "    # Step 1: Remove ambient light peak method 1 or 2\n",
    "    if FLAG_AMBIENT_LIGHT:\n",
    "        if FLAG_AMBIENT_LIGHT == 1:\n",
    "            # print(FLAG_AMBIENT_LIGHT)\n",
    "            data = np.array(Dataset_df['Data'].tolist())\n",
    "            data = removeAmbientLight_method1(data) # Method 1: Remove just the peak of the ambient light\n",
    "        if FLAG_AMBIENT_LIGHT == 2:\n",
    "            # print(FLAG_AMBIENT_LIGHT)\n",
    "            data = removeAmbientLight_method2(Dataset_df,ambient_baseline_df) # Method 2: Estimate the ambient light for each scan\n",
    "    else:\n",
    "        data = np.array(Dataset_df['Data'].tolist())\n",
    "\n",
    "    # Step 2: Crop the data to 420 nm\n",
    "    if FLAG_CROP:\n",
    "        data = data[:,280:,:]\n",
    "\n",
    "    # Step 3: Normalize the data using minimax\n",
    "    if FLAG_NORMALIZE: \n",
    "        data = process.normalize(data)\n",
    "\n",
    "    # Step 4: Divide by the baseline transfer function\n",
    "    if FLAG_TFUNC:\n",
    "        data = divTfuc(data,tFunc)\n",
    "\n",
    "    # Step 5: Feature reduction\n",
    "\n",
    "    processed_data = data.copy()\n",
    "\n",
    "    # Step 6: Turn the processed data into a singe data column in a dataframe\n",
    "    data_df = pd.DataFrame()\n",
    "    for i in range(processed_data.shape[0]):\n",
    "        new_row = {'Data_preprocessed':processed_data[i,:,:]}\n",
    "        data_df = pd.concat([data_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    # Add the data to the dataframe\n",
    "    processedData_df = pd.concat([Dataset_df, data_df], axis=1)\n",
    "    return processedData_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebdde98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAmbientLight_method2(Dataset_df, baseline_df):\n",
    "    Dataset = []\n",
    "    for i in range(Dataset_df.shape[0]):\n",
    "        # Get the data\n",
    "        data = np.array(Dataset_df['Data'].tolist())[i,:,:]\n",
    "        # Get the sampleID\n",
    "        sampleID = Dataset_df['SampleID'][i]\n",
    "        # Get the label\n",
    "        label = Dataset_df['Label (numeric)'][i]\n",
    "        # print(label)\n",
    "        # Find the corresponding baseline ambient light and the baseline ratio\n",
    "        # Get the row of baseline_df where baseline_df['sampleID'] == sampleID and \n",
    "        baseline_sample_row = baseline_df.loc[(baseline_df['SampleID'] == sampleID)]\n",
    "        # Get the row of baseline_df where baseline_df['Label (numeric)'] == Label (numeric)\n",
    "        # print(baseline_sample_row['Label (numeric)']=='1')\n",
    "        # print (baseline_sample_row['Label (numeric)'][0])\n",
    "        # print (str(label))\n",
    "        # display(baseline_sample_row)\n",
    "        baseline_row = baseline_sample_row.loc[(baseline_sample_row['Label (numeric)'] == label)]\n",
    "        # display(baseline_row)\n",
    "        # print(baseline_row)\n",
    "        # if baseline_row is empty give an error\n",
    "        if baseline_row.empty:\n",
    "            print(\"Error: baseline_row is empty\")\n",
    "            break\n",
    "        \n",
    "        # Get the baseline ambient light\n",
    "        ambient_baseline = np.array(baseline_row['Baseline Ambient Light'].tolist())[0,:,:]\n",
    "        # Get the baseline ratio\n",
    "        ratio_baseline = np.array(baseline_row['Baseline Ratio'].tolist())[0]\n",
    "\n",
    "        # Remove ambient light peak\n",
    "        data = removeAmbientLight1(data,ambient_baseline,ratio_baseline)\n",
    "\n",
    "        Dataset.append(data)\n",
    "\n",
    "        # Normalize the data\n",
    "        # data = process.normalize(data)\n",
    "        # Display the data\n",
    "        # GUI.plotSpectra(xdata=data[:,0],ydata=data[:,1],xlab='Wavelength(nm)',ylab='Reflected Intensity', title='Signal Spectrum after subtracting ambient light')\n",
    "    # Return the dataset as an array\n",
    "    Dataset = np.array(Dataset,dtype='float')\n",
    "    return Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f2984e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_LDA(train_df, data_column):\n",
    "    # print('Name of data column for LDA: ',data_column)\n",
    "    # Extract train data\n",
    "    if data_column == 'Data_preprocessed':\n",
    "        train_X = np.array(train_df[data_column].tolist())[:, :, 1]\n",
    "    else:\n",
    "        train_X = np.array(train_df[data_column].tolist())\n",
    "    train_y = np.array(train_df['Label (numeric)'].tolist())\n",
    "    # print('Shape of the data:',train_X.shape)\n",
    "    # fit the model\n",
    "    # lda_classifier = LDA(solver='lsqr', shrinkage='auto')\n",
    "    # lda_classifier = LDA(solver = 'svd')\n",
    "    lda_classifier = LDA()\n",
    "    lda_classifier.fit(train_X, train_y)\n",
    "    return lda_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c6f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAmbientLight_method1(data):\n",
    "    start_index = 1110\n",
    "    width = 20\n",
    "    end_index = start_index + width\n",
    "    for i in range(len(data)):\n",
    "        data[i,start_index:end_index,1] = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b949f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divTfuc(inputData,tFunc):    \n",
    "    outputData = inputData.copy()\n",
    "    # For each spectra\n",
    "    for i in range (inputData[:,:,1].shape[0]):\n",
    "        data = inputData[i,:,1]\n",
    "        # Divide by the baseline transfer function\n",
    "        outputData[i,:,1] = data / tFunc \n",
    "    outputData = process.normalize(outputData)\n",
    "    return outputData\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5de0c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Flags:\n",
      "FLAG_AMBIENT_LIGHT: 1\n",
      "FLAG_CROP: True\n",
      "FLAG_NORMALIZE: True\n",
      "FLAG_TFUNC: True\n",
      "Dataset_df shape:  (399, 6)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tFunc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\GithubProjects\\Spectroscopy_TrackedTissueSensing\\PrelimKidneyAnalysi_clustering.ipynb Cell 88\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GithubProjects/Spectroscopy_TrackedTissueSensing/PrelimKidneyAnalysi_clustering.ipynb#Y142sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# print the shape of the dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GithubProjects/Spectroscopy_TrackedTissueSensing/PrelimKidneyAnalysi_clustering.ipynb#Y142sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDataset_df shape: \u001b[39m\u001b[39m'\u001b[39m, Dataset_df\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/GithubProjects/Spectroscopy_TrackedTissueSensing/PrelimKidneyAnalysi_clustering.ipynb#Y142sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m processedData_df \u001b[39m=\u001b[39m preprocessing_pipeline(Dataset_df,ambient_baseline_df, FLAG_AMBIENT_LIGHT,FLAG_CROP, FLAG_NORMALIZE, FLAG_TFUNC)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GithubProjects/Spectroscopy_TrackedTissueSensing/PrelimKidneyAnalysi_clustering.ipynb#Y142sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m data_df \u001b[39m=\u001b[39m processedData_df\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GithubProjects/Spectroscopy_TrackedTissueSensing/PrelimKidneyAnalysi_clustering.ipynb#Y142sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Add the sample number to the dataframe\u001b[39;00m\n",
      "\u001b[1;32mc:\\GithubProjects\\Spectroscopy_TrackedTissueSensing\\PrelimKidneyAnalysi_clustering.ipynb Cell 88\u001b[0m in \u001b[0;36mpreprocessing_pipeline\u001b[1;34m(Dataset_df, ambient_baseline_df, FLAG_AMBIENT_LIGHT, FLAG_CROP, FLAG_NORMALIZE, FLAG_TFUNC)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GithubProjects/Spectroscopy_TrackedTissueSensing/PrelimKidneyAnalysi_clustering.ipynb#Y142sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Step 4: Divide by the baseline transfer function\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GithubProjects/Spectroscopy_TrackedTissueSensing/PrelimKidneyAnalysi_clustering.ipynb#Y142sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mif\u001b[39;00m FLAG_TFUNC:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/GithubProjects/Spectroscopy_TrackedTissueSensing/PrelimKidneyAnalysi_clustering.ipynb#Y142sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     data \u001b[39m=\u001b[39m divTfuc(data,tFunc)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GithubProjects/Spectroscopy_TrackedTissueSensing/PrelimKidneyAnalysi_clustering.ipynb#Y142sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Step 5: Feature reduction\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GithubProjects/Spectroscopy_TrackedTissueSensing/PrelimKidneyAnalysi_clustering.ipynb#Y142sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m processed_data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tFunc' is not defined"
     ]
    }
   ],
   "source": [
    "''' Preprocessing Pipeline \n",
    "'''\n",
    "# Preprocessing flags\n",
    "FLAG_NORMALIZE = True       # True, False\n",
    "FLAG_AMBIENT_LIGHT = 1   # None, 1, 2\n",
    "FLAG_CROP = True           # True, False\n",
    "FLAG_TFUNC = True          # True, False\n",
    "\n",
    "# print all the flags used\n",
    "print('Preprocessing Flags:')\n",
    "print('FLAG_AMBIENT_LIGHT:',FLAG_AMBIENT_LIGHT)\n",
    "print('FLAG_CROP:',FLAG_CROP)\n",
    "print('FLAG_NORMALIZE:',FLAG_NORMALIZE)\n",
    "print('FLAG_TFUNC:',FLAG_TFUNC)\n",
    "\n",
    "# print the shape of the dataset\n",
    "print('Dataset_df shape: ', Dataset_df.shape)\n",
    "processedData_df = preprocessing_pipeline(Dataset_df,ambient_baseline_df, FLAG_AMBIENT_LIGHT,FLAG_CROP, FLAG_NORMALIZE, FLAG_TFUNC)\n",
    "\n",
    "data_df = processedData_df.copy()\n",
    "# Add the sample number to the dataframe\n",
    "data_df['Sample Number'] = data_df['SampleID'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# print\n",
    "# print the details of the Dataset_df\n",
    "print('Dataset_df shape: ', data_df.shape)\n",
    "display(data_df.head())\n",
    "\n",
    "# Plot an example of the data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data_df['Data_preprocessed'][1][:,0], data_df['Data_preprocessed'][1][:,1])\n",
    "plt.title('Example of the data')\n",
    "plt.xlabel('Time (s)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d9b1e0b",
   "metadata": {},
   "source": [
    "### Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2638d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_data(data, wavelengths, num_points):\n",
    "    data = data.copy()\n",
    "    # Compute the number of data points for the desired resolution\n",
    "    resampled_wavelengths = np.linspace(wavelengths[0], wavelengths[-1], num=num_points)\n",
    "    # Resample the data to the desired resolution using linear interpolation\n",
    "    resampled_data = signal.resample(data, num_points)\n",
    "\n",
    "    return resampled_wavelengths, resampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' def evaluate_classifier \n",
    "This function is used to evalute a trained classifier using various metrics\n",
    "Inputs: testData_df: a dataframe containing the test data\n",
    "        classifier: a trained classifier\n",
    "        expName: a string containing the name of the experiment\n",
    "        average: a string containing the type of averaging to use for the precision and recall metrics\n",
    "        BY_SAMPLE: a boolean indicating whether to calculate the metrics by sample or not\n",
    "        CM_FLAG: a boolean indicating whether to plot the confusion matrix or not\n",
    "Outputs: results_df: a dataframe containing the results of the experiment\n",
    "'''\n",
    "\n",
    "# Import balanced accuracy score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def evaluate_classifier(testData_df, classifier, data_column = 'Data_preprocessed',expName='Specify classifier type', average='binary', BY_SAMPLE=False, CM_FLAG=True):\n",
    "    # print(f'{expName} Results:')\n",
    "    # print('Evaluating on data column:',data_column)\n",
    "    # Extract Test data\n",
    "    if data_column == 'Data_preprocessed':\n",
    "        test_X = np.array(testData_df['Data_preprocessed'].tolist())[:, :, 1]\n",
    "    else:\n",
    "        test_X = np.array(testData_df[data_column].tolist())\n",
    "    test_y = np.array(testData_df['Label (numeric)'].tolist())\n",
    "    # print('Shape of the data:',test_X.shape)\n",
    "\n",
    "    # Evaluate the performance of the classifier on the test data\n",
    "    y_pred = classifier.predict(test_X)\n",
    "\n",
    "    # Add the predicted labels to the dataframe\n",
    "    testData_df['Predicted Label'] = y_pred # I think this is the source of the warning\n",
    "    # Print the unique predicted labels\n",
    "    # print('Unique predicted labels:',np.unique(y_pred))\n",
    "\n",
    "\n",
    "    # Calculate all of the desired metrics: Accuracy, Precision, Recall, F1\n",
    "    accuracy = accuracy_score(test_y, y_pred)\n",
    "    precision = precision_score(test_y, y_pred, average=average)\n",
    "    recall = recall_score(test_y, y_pred, average=average)\n",
    "    f1 = f1_score(test_y, y_pred, average=average)\n",
    "    # Calculate the weighted metrics. This is useful when the classes are imbalanced\n",
    "    balanced_accuracy = balanced_accuracy_score(test_y, y_pred)\n",
    "\n",
    "\n",
    "    # display the calculated results in an aesthetic table\n",
    "    results_df = pd.DataFrame({'Balanced accuracy': [balanced_accuracy], 'Precision' : [precision], 'Recall' : [recall], 'F1' : [f1]}).round({'Value': 2})\n",
    "    \n",
    "\n",
    "    if CM_FLAG:\n",
    "        # display(results_df)\n",
    "        # Display the overall confusion matrix\n",
    "        cm = confusion_matrix(test_y, y_pred)\n",
    "        # show shape of cm\n",
    "        # print('Confusion Matrix Shape:',cm.shape)\n",
    "        unique_testy = np.unique(test_y)\n",
    "        unique_predy = np.unique(y_pred)\n",
    "        # print('Unique test_y:',unique_testy)\n",
    "        # print('Unique y_pred:',unique_predy)\n",
    "        classes = np.unique(np.concatenate((unique_testy,unique_predy)))\n",
    "        # classes = ['normal', 'cancer']\n",
    "        # classes = [1,2, 3]\n",
    "        cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "        # display(cm)\n",
    "        displayCM(cm_df, expName=expName)\n",
    "\n",
    "    if BY_SAMPLE:\n",
    "        # Display the confusion matrix broken down by sampleID\n",
    "        cm_dfs, sampleIDs = cmSampleBreakdown(testData_df)\n",
    "        for i in range(len(cm_dfs)):\n",
    "            displayCM(cm_dfs[i], expName=f'{expName}: {sampleIDs[i]}')\n",
    "    return results_df\n",
    "\n",
    "def displayCM(cm_df, expName='PCA-LDA'):\n",
    "    print(cm_df)\n",
    "    plt.figure()\n",
    "    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
    "    # create another sns.heatmate without a colourbar\n",
    "\n",
    "    # plt.title(f'{expName} Confusion Matrix')\n",
    "    # update the size of the text\n",
    "    plt.rcParams.update({'font.size': 25})\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('Actual Label')\n",
    "    # Remove the colour bar from the side of the colour bar\n",
    "    plt.show()\n",
    "\n",
    "# # create a cm_df\n",
    "# # cm_df = pd.DataFrame([[24,1],[39,87]], index=['0','1'], columns=['0','1'])\n",
    "# cm_df = pd.DataFrame([[0,0],[0,112]], index=['0','1'], columns=['0','1'])\n",
    "# print(cm_df)\n",
    "# displayCM(cm_df, expName='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e0f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' FEATURE REDUCTION '''\n",
    "\n",
    "def feature_reduction(train_df,test_df,FLAG_FEATURE_REDUCTION_METHOD,reduced_points=100, CM_FLAG = True):\n",
    "    # Print the shape of the train and test data\n",
    "    # print('Shape of the train data:', train_df.shape)\n",
    "    # print('Shape of the test data:', test_df.shape)\n",
    "\n",
    "\n",
    "    # CM_FLAG = True\n",
    "    # print('Feature Reduction Method:',FLAG_FEATURE_REDUCTION_METHOD)\n",
    "    if FLAG_FEATURE_REDUCTION_METHOD == 'None':\n",
    "        train_df['Data_reduced'] = train_df['Data_preprocessed']\n",
    "        test_df['Data_reduced'] = test_df['Data_preprocessed']\n",
    "        # fit the model\n",
    "        lda_classifier = fit_LDA(train_df, data_column = 'Data_preprocessed')\n",
    "        # evaluate the model\n",
    "        results_df = evaluate_classifier(testData_df=test_df, classifier=lda_classifier, expName='LDA: Baseline',CM_FLAG=CM_FLAG)\n",
    "    elif FLAG_FEATURE_REDUCTION_METHOD == 'PCA':\n",
    "        train_X = np.array(train_df['Data_preprocessed'].tolist())[:, :, 1]\n",
    "        pca = PCA(n_components=reduced_points)\n",
    "        pca.fit_transform(train_X)\n",
    "        # Add the reduced data to the train_df\n",
    "        train_X_reduced = pca.transform(train_X)\n",
    "        train_df['Data_reduced'] = [train_X_reduced[i] for i in range(train_X_reduced.shape[0])]\n",
    "        # Add the reduced data to the test_df\n",
    "        test_X = np.array(test_df['Data_preprocessed'].tolist())[:, :, 1]\n",
    "        test_X_reduced = pca.transform(test_X)\n",
    "        test_df['Data_reduced'] = [test_X_reduced[i] for i in range(test_X_reduced.shape[0])]\n",
    "        ### Fitting Classifier\n",
    "        lda = fit_LDA(train_df, data_column='Data_reduced')\n",
    "        ### Evaluation\n",
    "        results_df = evaluate_classifier(testData_df=test_df, classifier=lda, data_column = 'Data_reduced', expName='PCA-LDA: 100 components',CM_FLAG=CM_FLAG) \n",
    "    elif FLAG_FEATURE_REDUCTION_METHOD == 'Binning':\n",
    "        train_X = np.array(train_df['Data_preprocessed'].tolist())[:, :, 1]\n",
    "        test_X = np.array(test_df['Data_preprocessed'].tolist())[:, :, 1]\n",
    "        # Get the wavelengths for each spectrum\n",
    "        train_X_wavelengths = np.array(train_df['Data_preprocessed'].tolist())[:, :, 0]\n",
    "        test_X_wavelengths = np.array(test_df['Data_preprocessed'].tolist())[:, :, 0]\n",
    "\n",
    "        # Downsample the data to reduced_points points\n",
    "        # loop through the data and downsample each spectrum\n",
    "        num_points = reduced_points\n",
    "        # make an empty array to store the downsampled data\n",
    "        train_X_reduced = np.zeros((train_X.shape[0], num_points))\n",
    "        # print('Shape of the train data before downsampling:', train_X.shape[0])\n",
    "        for i in range(train_X.shape[0]):\n",
    "            train_X_reduced[i, :] = downsample_data(train_X[i, :], train_X_wavelengths[i, :], num_points=num_points)[1]\n",
    "        # print('Shape of the data after downsampling:',train_X.shape)\n",
    "        # Add the reduced data to the train_df\n",
    "        train_df['Data_reduced'] = [train_X_reduced[i] for i in range(train_X_reduced.shape[0])]\n",
    "        ### Fitting Classifier\n",
    "        lda = fit_LDA(train_df, data_column='Data_reduced')\n",
    "        ### Evaluation\n",
    "        # Downsample the test set\n",
    "        \n",
    "        test_X_wavelengths = np.array(test_df['Data_preprocessed'].tolist())[:, :, 0]\n",
    "        test_X_reduced = np.zeros((test_X.shape[0], num_points))\n",
    "        # print('Shape of the test data before downsampling:', test_X.shape)\n",
    "        for i in range(test_X.shape[0]):\n",
    "            test_X_reduced[i, :] = downsample_data(test_X[i, :], test_X_wavelengths[i, :], num_points=num_points)[1]\n",
    "        # add the reduced data to the test_df\n",
    "        test_df['Data_reduced'] = [test_X_reduced[i] for i in range(test_X_reduced.shape[0])]\n",
    "        results_df = evaluate_classifier(testData_df=test_df, classifier=lda, data_column='Data_reduced',expName='Binning-LDA: 100 bins',CM_FLAG=CM_FLAG)\n",
    "    elif FLAG_FEATURE_REDUCTION_METHOD == 'Feature_Selection':\n",
    "        # Step 1: Get the train and test data\n",
    "        train_X = np.array(train_df['Data_preprocessed'].tolist())[:, :, 1]\n",
    "        test_X = np.array(test_df['Data_preprocessed'].tolist())[:, :, 1]\n",
    "        # Step 2: Fit the LDA classifier to the data\n",
    "        lda_classifier = fit_LDA(train_df, data_column='Data_preprocessed')\n",
    "        \n",
    "        # Step 3: Extract the most discriminative features\n",
    "        feature_importances = lda_classifier.coef_[0]\n",
    "        # Step 4: Sort the feature importances and select the top reduced_points features\n",
    "        num_features = reduced_points\n",
    "        # np.argsort(feature_importances) sorts the array feature_importances in ascending order and returns the indices that would sort the array.\n",
    "        top_indices = np.argsort(feature_importances)[::-1][:num_features]\n",
    "        # top_indices = np.argsort(abs(feature_importances))[::-1][:num_features] % Makes it worse\n",
    "\n",
    "\n",
    "        # show an example of the features\n",
    "        selected_wavelengths = np.array(processedData_df['Data_preprocessed'].tolist())[0, :, 0][top_indices]\n",
    "        selected_features = np.array(processedData_df['Data_preprocessed'].tolist())[0, :, 1][top_indices]\n",
    "\n",
    "        # fig = go.Figure(data=go.Scatter(x=selected_wavelengths, y=selected_features, mode='markers'))\n",
    "        # fig.update_layout(\n",
    "        #     xaxis=dict(title='Wavelength (nm)'),\n",
    "        #     yaxis=dict(title='Intensity (a.u.)'),\n",
    "        #     title=f'Top {num_features} Selected Wavelengths',\n",
    "        #     width=figSize[0],\n",
    "        #     height=figSize[1]\n",
    "        # )\n",
    "        # fig.show()\n",
    "\n",
    "        # Step 5: Reduce the train and test data to the selected features\n",
    "        train_X_top = np.array(train_df['Data_preprocessed'].tolist())[:, :, 1][:, top_indices]\n",
    "        test_X_top = np.array(test_df['Data_preprocessed'].tolist())[:, :, 1][:, top_indices]\n",
    "\n",
    "        # Step 5.1 Add the reduced data to the train_df\n",
    "        train_df['Data_reduced'] = [train_X_top[i] for i in range(train_X_top.shape[0])]\n",
    "        # Step 5.2 Add the reduced data to the test_df\n",
    "        test_df['Data_reduced'] = [test_X_top[i] for i in range(test_X_top.shape[0])]\n",
    "\n",
    "        # Step 6: Fit the classifier to the reduced data\n",
    "        lda_classifier = fit_LDA(train_df,data_column='Data_reduced')\n",
    "\n",
    "        # Step 7: Evaluate the classifier on the test set\n",
    "        results_df = evaluate_classifier(testData_df=test_df, classifier=lda_classifier, data_column='Data_reduced', expName='LDA: Feature Selection',CM_FLAG=CM_FLAG)\n",
    "    return results_df\n",
    "\n",
    "# # feature_reduction(train_df,test_df,FLAG_FEATURE_REDUCTION_METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddb797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TRAIN TEST SPLIT '''\n",
    "\n",
    "TRAIN_TEST_SPLIT_METHOD = 'sample' # random, sampleID, sample\n",
    "FLAG_FEATURE_REDUCTION_METHOD = 'Binning' # None, PCA, Binning, Feature_Selection\n",
    "reduced_points = 100\n",
    "\n",
    "# print all the flags used\n",
    "print('Preprocessing Flags:')\n",
    "print('FLAG_AMBIENT_LIGHT:',FLAG_AMBIENT_LIGHT)\n",
    "print('FLAG_CROP:',FLAG_CROP)\n",
    "print('FLAG_NORMALIZE:',FLAG_NORMALIZE)\n",
    "print('FLAG_TFUNC:',FLAG_TFUNC)\n",
    "print('TRAIN_TEST_SPLIT_METHOD:',TRAIN_TEST_SPLIT_METHOD)\n",
    "print('FLAG_FEATURE_REDUCTION_METHOD:',FLAG_FEATURE_REDUCTION_METHOD)\n",
    "\n",
    "# run i times to find average and std\n",
    "balanced_accuracys = []\n",
    "F1s = []\n",
    "for i in range(1):\n",
    "# for i in range(1,51):    \n",
    "    if TRAIN_TEST_SPLIT_METHOD == 'random':\n",
    "        # for random_state in range(1,6):\n",
    "            # zebra = random_state\n",
    "            # print('Random State:',zebra)\n",
    "            train_df, test_df = train_test_split(data_df, test_size=0.3, random_state=i)\n",
    "\n",
    "            train_df.reset_index(drop=True, inplace=True)\n",
    "            test_df.reset_index(drop=True, inplace=True)\n",
    "            # Feature reduction and classification\n",
    "            results_df = feature_reduction(train_df,test_df,FLAG_FEATURE_REDUCTION_METHOD,reduced_points,CM_FLAG = False)\n",
    "    elif TRAIN_TEST_SPLIT_METHOD == 'sample':\n",
    "        # Get the unique Sample Numbers\n",
    "        unique_ids = data_df['Sample Number'].unique()\n",
    "        # print('Unique Sample Numbers:',unique_ids)\n",
    "        # Perform a train test split based on sample number\n",
    "        result_all = []\n",
    "        for i in range(len(unique_ids)):\n",
    "            print(\"Test set is sample number:\",unique_ids[i])\n",
    "            # get the train samples\n",
    "            train_df = data_df[data_df['Sample Number'] != unique_ids[i]].reset_index(drop=True)\n",
    "            # get the test sample\n",
    "            test_df = data_df[data_df['Sample Number'] == unique_ids[i]].reset_index(drop=True)\n",
    "            results_df = feature_reduction(train_df,test_df,FLAG_FEATURE_REDUCTION_METHOD, reduced_points,CM_FLAG=False)\n",
    "            result_all.append(results_df)\n",
    "        # concatenate all of the results\n",
    "        results_df = pd.concat(result_all).reset_index(drop=True)\n",
    "        display(results_df)\n",
    "        # Get the average and standard deviation of each column\n",
    "        results_df_mean = results_df.mean(axis=0).to_frame().T\n",
    "        results_df_mean['Metric'] = 'Mean'  \n",
    "        results_df_std = results_df.std(axis=0).to_frame().T\n",
    "        results_df_std['Metric'] = 'Std'\n",
    "        results_df = pd.concat([results_df_mean,results_df_std]).reset_index(drop=True)\n",
    "        display(results_df)\n",
    "    elif TRAIN_TEST_SPLIT_METHOD == 'sampleID':\n",
    "        # Get the unique sample IDs\n",
    "        unique_ids = data_df['SampleID'].unique()\n",
    "        # Perform a train test split based on sampleID\n",
    "        result_all = []\n",
    "        for i in range(len(unique_ids)):\n",
    "            # get the test sample \n",
    "            test_df = data_df[data_df['SampleID'] == unique_ids[i]].reset_index(drop=True)\n",
    "            # get the train samples\n",
    "            train_df = data_df[data_df['SampleID'] != unique_ids[i]].reset_index(drop=True)\n",
    "            results_df = feature_reduction(train_df,test_df,FLAG_FEATURE_REDUCTION_METHOD, reduced_points, CM_FLAG=False)\n",
    "            result_all.append(results_df)\n",
    "        # concatenate all of the results\n",
    "        results_df = pd.concat(result_all).reset_index(drop=True)\n",
    "        display(results_df)\n",
    "        # Get the average and standard deviation of each column\n",
    "        results_df_mean = results_df.mean(axis=0).to_frame().T\n",
    "        results_df_mean['Metric'] = 'Mean'  \n",
    "        results_df_std = results_df.std(axis=0).to_frame().T\n",
    "        results_df_std['Metric'] = 'Std'\n",
    "        results_df = pd.concat([results_df_mean,results_df_std]).reset_index(drop=True)\n",
    "        display(results_df)\n",
    "    # extract the balanced accuracy\n",
    "    # list the column names of the results_df\n",
    "    # print('Column names:',results_df.columns)\n",
    "    balanced_accuracy = results_df['Balanced accuracy'][0]\n",
    "    balanced_accuracys.append(balanced_accuracy)\n",
    "    F1 = results_df['F1'][0]\n",
    "    F1s.append(F1)\n",
    "# RESULTS\n",
    "print('Average Balanced Accuracy:',np.mean(balanced_accuracys))\n",
    "print('Standard Deviation:',np.std(balanced_accuracys))\n",
    "\n",
    "print('Average F1:',np.mean(F1s))\n",
    "print('Standard Deviation:',np.std(F1s))\n",
    "\n",
    "# # Plot the balanced accuracy \n",
    "# fig = go.Figure(data=go.Scatter(x=np.arange(len(balanced_accuracys)), y=balanced_accuracys, mode='markers'))\n",
    "# fig.update_layout(\n",
    "#     xaxis=dict(title='Iteration'),\n",
    "#     yaxis=dict(title='Balanced Accuracy'),\n",
    "#     title=f'Balanced Accuracy for {TRAIN_TEST_SPLIT_METHOD} train test split',\n",
    "#     width=figSize[0],\n",
    "#     height=figSize[1]\n",
    "# )\n",
    "# fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f4ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "84d8cac4d95fdd2ab02498a6ec40a50cb9882041e67cb52e6d8bcfda00d28db9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
